{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Computer Vision Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data\n",
    "\n",
    "train_data = datasets.FashionMNIST(root='data', train= True, transform= ToTensor(), download= True)\n",
    "test_data = datasets.FashionMNIST(root='data', train= False, transform= ToTensor(), download= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the length of training data is 60000, length of test data  10000\n"
     ]
    }
   ],
   "source": [
    "print(f\" the length of training data is {len(train_data)}, length of test data  { len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0863, 0.4627, 0.0941, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1882, 0.3451, 0.0196, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0471, 0.3922, 0.8314, 0.8039, 0.7255, 0.7020, 0.6784, 0.7294,\n",
       "           0.7569, 0.8667, 0.5569, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.3333, 0.2980, 0.7804, 0.8824, 0.9725, 1.0000, 0.9333,\n",
       "           0.8863, 0.6157, 0.2667, 0.3137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.3569, 0.2706, 0.3569, 0.7882, 0.8549, 0.8824, 0.8196,\n",
       "           0.6196, 0.2392, 0.3647, 0.2824, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.3098, 0.3490, 0.2392, 0.2314, 0.3412, 0.4235, 0.2941,\n",
       "           0.2196, 0.2980, 0.3804, 0.2863, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2941, 0.3490, 0.3137, 0.3137, 0.2627, 0.2471, 0.2863,\n",
       "           0.3255, 0.3137, 0.3765, 0.2824, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.3020, 0.3451, 0.3020, 0.3137, 0.3255, 0.3255, 0.3255,\n",
       "           0.3255, 0.3176, 0.3725, 0.2980, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.3490, 0.3765, 0.3137, 0.3255, 0.3176, 0.3294, 0.3333,\n",
       "           0.3333, 0.3333, 0.3804, 0.3294, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.3647, 0.3804, 0.3176, 0.3333, 0.3294, 0.3333, 0.3412,\n",
       "           0.3451, 0.3294, 0.3882, 0.3412, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.3725, 0.3412, 0.3294, 0.3412, 0.3451, 0.3333, 0.3412,\n",
       "           0.3412, 0.3294, 0.3608, 0.3412, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.3804, 0.3412, 0.3412, 0.3333, 0.3451, 0.3412, 0.3412,\n",
       "           0.3412, 0.3451, 0.3333, 0.4196, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0667, 0.3922, 0.3451, 0.3412, 0.3412, 0.3451, 0.3412, 0.3412,\n",
       "           0.3333, 0.3490, 0.3020, 0.4627, 0.0314, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0392, 0.3647, 0.3412, 0.3412, 0.3412, 0.3412, 0.3412, 0.3451,\n",
       "           0.3412, 0.3490, 0.3137, 0.4039, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0353, 0.3765, 0.3412, 0.3412, 0.3412, 0.3412, 0.3412, 0.3451,\n",
       "           0.3412, 0.3451, 0.3412, 0.4039, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0471, 0.3765, 0.3333, 0.3412, 0.3412, 0.3412, 0.3333, 0.3412,\n",
       "           0.3412, 0.3451, 0.3490, 0.3922, 0.0078, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0784, 0.3725, 0.3294, 0.3451, 0.3333, 0.3412, 0.3451, 0.3451,\n",
       "           0.3451, 0.3490, 0.3451, 0.3882, 0.0314, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0824, 0.3765, 0.3333, 0.3412, 0.3333, 0.3451, 0.3451, 0.3451,\n",
       "           0.3451, 0.3490, 0.3490, 0.3882, 0.0392, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0941, 0.3765, 0.3333, 0.3412, 0.3333, 0.3412, 0.3451, 0.3451,\n",
       "           0.3490, 0.3451, 0.3569, 0.4000, 0.0549, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0980, 0.3647, 0.3294, 0.3451, 0.3412, 0.3412, 0.3412, 0.3412,\n",
       "           0.3412, 0.3490, 0.3569, 0.4039, 0.1137, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.1176, 0.3725, 0.3333, 0.3451, 0.3451, 0.3412, 0.3412, 0.3412,\n",
       "           0.3412, 0.3490, 0.3451, 0.4000, 0.1451, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.1333, 0.3765, 0.3451, 0.3412, 0.3412, 0.3412, 0.3412, 0.3412,\n",
       "           0.3412, 0.3333, 0.3333, 0.3804, 0.1490, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.1569, 0.3765, 0.3412, 0.3333, 0.3412, 0.3412, 0.3412, 0.3412,\n",
       "           0.3412, 0.3333, 0.3294, 0.3608, 0.1922, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.1804, 0.3725, 0.3255, 0.3294, 0.3412, 0.3412, 0.3412, 0.3412,\n",
       "           0.3412, 0.3412, 0.3294, 0.3412, 0.3294, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.2824, 0.3725, 0.3333, 0.3294, 0.3333, 0.3451, 0.3412, 0.3412,\n",
       "           0.3490, 0.3412, 0.3333, 0.3255, 0.2471, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.2510, 0.3922, 0.3294, 0.3412, 0.3451, 0.3333, 0.3451, 0.3451,\n",
       "           0.3294, 0.3412, 0.3255, 0.3725, 0.2078, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0392, 0.4000, 0.3922, 0.3569, 0.3569, 0.3490, 0.3333, 0.3294,\n",
       "           0.3294, 0.3412, 0.4235, 0.4157, 0.0549, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0314, 0.2863, 0.3647, 0.4078, 0.4196, 0.4039, 0.4039,\n",
       "           0.4157, 0.4000, 0.2941, 0.0392, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0039, 0.0000, 0.0000, 0.0000, 0.0706, 0.1647, 0.2235, 0.2196,\n",
       "           0.1255, 0.0314, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " 0)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#visualize any image randomly\n",
    "img, label = train_data[2]\n",
    "img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classnames = train_data.classes\n",
    "classnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize the image\n",
    "\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.title(classnames[label])\n",
    "plt.axis(False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#multiple random images\n",
    "torch.manual_seed(42)\n",
    "fig  = plt.Figure(figsize=(9,9))\n",
    "rows, cols = 4, 4\n",
    "\n",
    "for i in range(1, rows*cols +1):\n",
    "    #pick image randomly\n",
    "    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
    "    img, label = train_data[random_idx]\n",
    "    fig.add_subplot(rows, cols, i)\n",
    "    plt.imshow(img.squeeze(), cmap = 'gray')\n",
    "    plt.axis(False)\n",
    "    plt.title(classnames[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The reduced training databatch is 1875\n",
      " The reduced test databatch is 313\n"
     ]
    }
   ],
   "source": [
    "#dataloader to loop through the dataset\n",
    "\n",
    "#hyperparameter\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_data_batch = DataLoader(train_data, batch_size = BATCH_SIZE, shuffle= True )\n",
    "\n",
    "test_data_batch = DataLoader(test_data, batch_size = BATCH_SIZE, shuffle= False)\n",
    "\n",
    "print(f\" The reduced training databatch is {len(train_data_batch)}\")\n",
    "print(f\" The reduced test databatch is {len(test_data_batch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 28, 28]), torch.Size([32]))"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out what's inside the training dataloader\n",
    "train_features_batch, train_labels_batch = next(iter(train_data_batch))\n",
    "train_features_batch.shape, train_labels_batch.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTVO(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_layers: int,  output_shape: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_stack = nn.Sequential( nn.Flatten(),\n",
    "                                          nn.Linear(in_features= input_shape, out_features= hidden_layers),\n",
    "                                          nn.Linear(in_features= hidden_layers, out_features= output_shape),\n",
    "                                          )\n",
    "        \n",
    "\n",
    "    #define the forward method\n",
    "    def forward(self, x):\n",
    "        return self.linear_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTVO(\n",
       "  (linear_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a model for the class\n",
    "\n",
    "model_0 = FashionMNISTVO(input_shape= 784,\n",
    "                         hidden_layers= 10,\n",
    "                         output_shape=len(classnames)).to('cpu')\n",
    "\n",
    "model_0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup loss, optimizer and evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_0.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Skipping the download the file already exists\n"
     ]
    }
   ],
   "source": [
    "#import helper functions\n",
    "\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "r = requests.get(url='https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py')\n",
    "\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "    print(\" Skipping the download the file already exists\")\n",
    "else:\n",
    "    print('Downloading file...........')\n",
    "\n",
    "    with open('helper_functions.py', 'wb') as f:\n",
    "\n",
    "        f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time function\n",
    "\n",
    "def my_run_time(start_time, end_time):\n",
    "\n",
    "    return f\" the elapsed time is { end_time - start_time} seconds\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is epoch number 0\n",
      "the batches checked are 0\n",
      "the batches checked are 16000\n",
      "the batches checked are 32000\n",
      "the batches checked are 48000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:07<00:15,  7.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the training loss is 0.000230        <||||     test loss 0.508077\n",
      " this is epoch number 1\n",
      "the batches checked are 0\n",
      "the batches checked are 16000\n",
      "the batches checked are 32000\n",
      "the batches checked are 48000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:15<00:07,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the training loss is 0.000315        <||||     test loss 0.481833\n",
      " this is epoch number 2\n",
      "the batches checked are 0\n",
      "the batches checked are 16000\n",
      "the batches checked are 32000\n",
      "the batches checked are 48000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:24<00:00,  8.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the training loss is 0.000161        <||||     test loss 0.478955\n",
      " the elapsed time is 24.903162717819214 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "#set the manual seed\n",
    "torch.manual_seed(42)\n",
    "#set the timer\n",
    "start_time = time.time()\n",
    "\n",
    "#set the epochs\n",
    "epochs = 3\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    print(f\" this is epoch number {epoch}\")\n",
    "\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch, (X,y) in enumerate(train_data_batch):\n",
    "\n",
    "        #train model\n",
    "        model_0.train()\n",
    "\n",
    "        #do the forward pass\n",
    "        y_train_pred = model_0(X)\n",
    "\n",
    "        #calculate the loss\n",
    "        loss = loss_fn(y_train_pred, y)\n",
    "        train_loss += loss\n",
    "\n",
    "        #zero gradient\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        #optimze step\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 500 == 0:\n",
    "            print(f\"the batches checked are {batch * len(X)}\")\n",
    "\n",
    "        train_loss /= len(train_data_batch)\n",
    "\n",
    "        #testing\n",
    "\n",
    "        test_loss, acc = 0, 0\n",
    "\n",
    "    model_0.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "\n",
    "            #do forward pass\n",
    "\n",
    "        for batch, (X,y) in enumerate(test_data_batch):\n",
    "                #do the forward pass\n",
    "            y_test = model_0(X)\n",
    "\n",
    "                #calculate the loss\n",
    "            t_loss = loss_fn(y_test, y)\n",
    "            test_loss += t_loss\n",
    "\n",
    "        test_loss /= len(test_data_batch)\n",
    "\n",
    "    print(f\" the training loss is {train_loss:.6f}        <||||     test loss {test_loss:.6f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "\n",
    "print(my_run_time(start_time, end_time))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Non-Linearity to our Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTV1(nn.Module):\n",
    "\n",
    "    def __init__(self, input_shape: int, hidden_layers : int, output_shape: int ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_stacktwo = nn.Sequential( nn.Flatten(),\n",
    "                                             nn.Linear(in_features= input_shape, out_features= hidden_layers),\n",
    "                                             nn.ReLU(),\n",
    "                                             nn.Linear(in_features=hidden_layers, out_features= output_shape),\n",
    "                                             nn.ReLU())\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.linear_stacktwo(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTV1(\n",
       "  (linear_stacktwo): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (4): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instantiate the class\n",
    "\n",
    "model_1 = FashionMNISTV1(input_shape=784,\n",
    "                         hidden_layers= 10,\n",
    "                         output_shape=len(classnames)).to(device='cpu')\n",
    "\n",
    "model_1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and Accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import accuracy_fn\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model_1.parameters(), lr = 0.1)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functionalizing the testing and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model: torch.nn.Module,\n",
    "                  optimizer: torch.optim.Optimizer,\n",
    "                  loss: torch.nn.Module,\n",
    "                  dataloader: torch.utils.data.DataLoader,\n",
    "                  device: torch.device = device):\n",
    "    \n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    train_acc, train_loss = 0, 0\n",
    "\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        #do the forward pass\n",
    "\n",
    "        y_pred = model(X)\n",
    "\n",
    "        #calculate the loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "\n",
    "        #optimizer\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        #optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 440 == 0:\n",
    "\n",
    "            print(f\"the train loss is {train_loss:.5f}     || \")\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module,\n",
    "                  \n",
    "                  loss_fn: torch.nn.Module,\n",
    "                  dataloader: torch.utils.data.DataLoader,\n",
    "                  device: torch.device = device):\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    #loop through the dataset data\n",
    "\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        for batch, (X,y) in enumerate(dataloader):\n",
    "\n",
    "            #forwward pass\n",
    "\n",
    "            y_test = model(X)\n",
    "\n",
    "            #calculate the loss\n",
    "\n",
    "            loss = loss_fn(y_test, y)\n",
    "            test_loss += loss\n",
    "\n",
    "            if batch % 440 == 0:\n",
    "                print(f\"the test loss is {test_loss:.5f}     || \")\n",
    "\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the train loss is 2.31190     || \n",
      "the train loss is 647.48639     || \n"
     ]
    }
   ],
   "source": [
    "#run a loop through all the data \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    #training data\n",
    "    training_step(model=model_1,\n",
    "                  optimizer= optimizer,\n",
    "                  loss = loss_fn,\n",
    "                  dataloader=train_data_batch,\n",
    "                  device= 'cpu' )\n",
    "    \n",
    "    test_step(model= model_1,\n",
    "              loss_fn= loss_fn,\n",
    "              dataloader= test_data_batch,\n",
    "              device= 'cpu')\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(my_run_time(start_time, end_time))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTV2(nn.Module):\n",
    "    #class constructor\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels= input_shape,\n",
    "                      out_channels= hidden_units,\n",
    "                      kernel_size= 3,\n",
    "                      stride= 1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d( in_channels = hidden_units,\n",
    "                      out_channels = hidden_units,\n",
    "                      kernel_size = 3,\n",
    "                      stride = 1,\n",
    "                      padding= 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size= 2))\n",
    "        \n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels= hidden_units,\n",
    "                      out_channels= hidden_units,\n",
    "                      kernel_size= 3,\n",
    "                      stride= 1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d( in_channels = hidden_units,\n",
    "                      out_channels = hidden_units,\n",
    "                      kernel_size = 3,\n",
    "                      stride = 1,\n",
    "                      padding= 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size= 2)            \n",
    "\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features= hidden_units*7*7, out_features= len(classnames), )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv_block_1(x)\n",
    "\n",
    "        x = self.conv_block_2(x)\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
