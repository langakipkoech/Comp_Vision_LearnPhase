{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Computer Vision Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data\n",
    "\n",
    "train_data = datasets.FashionMNIST(root='data', train= True, transform= ToTensor(), download= True)\n",
    "test_data = datasets.FashionMNIST(root='data', train= False, transform= ToTensor(), download= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the length of training data is 60000, length of test data  10000\n"
     ]
    }
   ],
   "source": [
    "print(f\" the length of training data is {len(train_data)}, length of test data  { len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0863, 0.4627, 0.0941, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1882, 0.3451, 0.0196, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0471, 0.3922, 0.8314, 0.8039, 0.7255, 0.7020, 0.6784, 0.7294,\n",
       "           0.7569, 0.8667, 0.5569, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.3333, 0.2980, 0.7804, 0.8824, 0.9725, 1.0000, 0.9333,\n",
       "           0.8863, 0.6157, 0.2667, 0.3137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.3569, 0.2706, 0.3569, 0.7882, 0.8549, 0.8824, 0.8196,\n",
       "           0.6196, 0.2392, 0.3647, 0.2824, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.3098, 0.3490, 0.2392, 0.2314, 0.3412, 0.4235, 0.2941,\n",
       "           0.2196, 0.2980, 0.3804, 0.2863, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2941, 0.3490, 0.3137, 0.3137, 0.2627, 0.2471, 0.2863,\n",
       "           0.3255, 0.3137, 0.3765, 0.2824, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.3020, 0.3451, 0.3020, 0.3137, 0.3255, 0.3255, 0.3255,\n",
       "           0.3255, 0.3176, 0.3725, 0.2980, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.3490, 0.3765, 0.3137, 0.3255, 0.3176, 0.3294, 0.3333,\n",
       "           0.3333, 0.3333, 0.3804, 0.3294, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.3647, 0.3804, 0.3176, 0.3333, 0.3294, 0.3333, 0.3412,\n",
       "           0.3451, 0.3294, 0.3882, 0.3412, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.3725, 0.3412, 0.3294, 0.3412, 0.3451, 0.3333, 0.3412,\n",
       "           0.3412, 0.3294, 0.3608, 0.3412, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.3804, 0.3412, 0.3412, 0.3333, 0.3451, 0.3412, 0.3412,\n",
       "           0.3412, 0.3451, 0.3333, 0.4196, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0667, 0.3922, 0.3451, 0.3412, 0.3412, 0.3451, 0.3412, 0.3412,\n",
       "           0.3333, 0.3490, 0.3020, 0.4627, 0.0314, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0392, 0.3647, 0.3412, 0.3412, 0.3412, 0.3412, 0.3412, 0.3451,\n",
       "           0.3412, 0.3490, 0.3137, 0.4039, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0353, 0.3765, 0.3412, 0.3412, 0.3412, 0.3412, 0.3412, 0.3451,\n",
       "           0.3412, 0.3451, 0.3412, 0.4039, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0471, 0.3765, 0.3333, 0.3412, 0.3412, 0.3412, 0.3333, 0.3412,\n",
       "           0.3412, 0.3451, 0.3490, 0.3922, 0.0078, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0784, 0.3725, 0.3294, 0.3451, 0.3333, 0.3412, 0.3451, 0.3451,\n",
       "           0.3451, 0.3490, 0.3451, 0.3882, 0.0314, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0824, 0.3765, 0.3333, 0.3412, 0.3333, 0.3451, 0.3451, 0.3451,\n",
       "           0.3451, 0.3490, 0.3490, 0.3882, 0.0392, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0941, 0.3765, 0.3333, 0.3412, 0.3333, 0.3412, 0.3451, 0.3451,\n",
       "           0.3490, 0.3451, 0.3569, 0.4000, 0.0549, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0980, 0.3647, 0.3294, 0.3451, 0.3412, 0.3412, 0.3412, 0.3412,\n",
       "           0.3412, 0.3490, 0.3569, 0.4039, 0.1137, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.1176, 0.3725, 0.3333, 0.3451, 0.3451, 0.3412, 0.3412, 0.3412,\n",
       "           0.3412, 0.3490, 0.3451, 0.4000, 0.1451, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.1333, 0.3765, 0.3451, 0.3412, 0.3412, 0.3412, 0.3412, 0.3412,\n",
       "           0.3412, 0.3333, 0.3333, 0.3804, 0.1490, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.1569, 0.3765, 0.3412, 0.3333, 0.3412, 0.3412, 0.3412, 0.3412,\n",
       "           0.3412, 0.3333, 0.3294, 0.3608, 0.1922, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.1804, 0.3725, 0.3255, 0.3294, 0.3412, 0.3412, 0.3412, 0.3412,\n",
       "           0.3412, 0.3412, 0.3294, 0.3412, 0.3294, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.2824, 0.3725, 0.3333, 0.3294, 0.3333, 0.3451, 0.3412, 0.3412,\n",
       "           0.3490, 0.3412, 0.3333, 0.3255, 0.2471, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.2510, 0.3922, 0.3294, 0.3412, 0.3451, 0.3333, 0.3451, 0.3451,\n",
       "           0.3294, 0.3412, 0.3255, 0.3725, 0.2078, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0392, 0.4000, 0.3922, 0.3569, 0.3569, 0.3490, 0.3333, 0.3294,\n",
       "           0.3294, 0.3412, 0.4235, 0.4157, 0.0549, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0314, 0.2863, 0.3647, 0.4078, 0.4196, 0.4039, 0.4039,\n",
       "           0.4157, 0.4000, 0.2941, 0.0392, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0039, 0.0000, 0.0000, 0.0000, 0.0706, 0.1647, 0.2235, 0.2196,\n",
       "           0.1255, 0.0314, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " 0)"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#visualize any image randomly\n",
    "img, label = train_data[2]\n",
    "img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classnames = train_data.classes\n",
    "classnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize the image\n",
    "\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.title(classnames[label])\n",
    "plt.axis(False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#multiple random images\n",
    "torch.manual_seed(42)\n",
    "fig  = plt.Figure(figsize=(9,9))\n",
    "rows, cols = 4, 4\n",
    "\n",
    "for i in range(1, rows*cols +1):\n",
    "    #pick image randomly\n",
    "    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
    "    img, label = train_data[random_idx]\n",
    "    fig.add_subplot(rows, cols, i)\n",
    "    plt.imshow(img.squeeze(), cmap = 'gray')\n",
    "    plt.axis(False)\n",
    "    plt.title(classnames[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The reduced training databatch is 1875\n",
      " The reduced test databatch is 313\n"
     ]
    }
   ],
   "source": [
    "#dataloader to loop through the dataset\n",
    "\n",
    "#hyperparameter\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_data_batch = DataLoader(train_data, batch_size = BATCH_SIZE, shuffle= True )\n",
    "\n",
    "test_data_batch = DataLoader(test_data, batch_size = BATCH_SIZE, shuffle= False)\n",
    "\n",
    "print(f\" The reduced training databatch is {len(train_data_batch)}\")\n",
    "print(f\" The reduced test databatch is {len(test_data_batch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 28, 28]), torch.Size([32]))"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out what's inside the training dataloader\n",
    "train_features_batch, train_labels_batch = next(iter(train_data_batch))\n",
    "train_features_batch.shape, train_labels_batch.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTVO(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_layers: int,  output_shape: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_stack = nn.Sequential( nn.Flatten(),\n",
    "                                          nn.Linear(in_features= input_shape, out_features= hidden_layers),\n",
    "                                          nn.Linear(in_features= hidden_layers, out_features= output_shape),\n",
    "                                          )\n",
    "        \n",
    "\n",
    "    #define the forward method\n",
    "    def forward(self, x):\n",
    "        return self.linear_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTVO(\n",
       "  (linear_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a model for the class\n",
    "\n",
    "model_0 = FashionMNISTVO(input_shape= 784,\n",
    "                         hidden_layers= 10,\n",
    "                         output_shape=len(classnames)).to('cpu')\n",
    "\n",
    "model_0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup loss, optimizer and evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_0.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Skipping the download the file already exists\n"
     ]
    }
   ],
   "source": [
    "#import helper functions\n",
    "\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "r = requests.get(url='https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py')\n",
    "\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "    print(\" Skipping the download the file already exists\")\n",
    "else:\n",
    "    print('Downloading file...........')\n",
    "\n",
    "    with open('helper_functions.py', 'wb') as f:\n",
    "\n",
    "        f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time function\n",
    "\n",
    "def my_run_time(start_time, end_time):\n",
    "\n",
    "    return f\" the elapsed time is { end_time - start_time} seconds\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is epoch number 0\n",
      "the batches checked are 0\n",
      "the batches checked are 16000\n",
      "the batches checked are 32000\n",
      "the batches checked are 48000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:07<00:15,  7.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the training loss is 0.000230        <||||     test loss 0.508077\n",
      " this is epoch number 1\n",
      "the batches checked are 0\n",
      "the batches checked are 16000\n",
      "the batches checked are 32000\n",
      "the batches checked are 48000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:15<00:07,  7.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the training loss is 0.000315        <||||     test loss 0.481833\n",
      " this is epoch number 2\n",
      "the batches checked are 0\n",
      "the batches checked are 16000\n",
      "the batches checked are 32000\n",
      "the batches checked are 48000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:25<00:00,  8.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the training loss is 0.000161        <||||     test loss 0.478955\n",
      " the elapsed time is 25.4294273853302 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "#set the manual seed\n",
    "torch.manual_seed(42)\n",
    "#set the timer\n",
    "start_time = time.time()\n",
    "\n",
    "#set the epochs\n",
    "epochs = 3\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    print(f\" this is epoch number {epoch}\")\n",
    "\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch, (X,y) in enumerate(train_data_batch):\n",
    "\n",
    "        #train model\n",
    "        model_0.train()\n",
    "\n",
    "        #do the forward pass\n",
    "        y_train_pred = model_0(X)\n",
    "\n",
    "        #calculate the loss\n",
    "        loss = loss_fn(y_train_pred, y)\n",
    "        train_loss += loss\n",
    "\n",
    "        #zero gradient\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        #optimze step\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 500 == 0:\n",
    "            print(f\"the batches checked are {batch * len(X)}\")\n",
    "\n",
    "        train_loss /= len(train_data_batch)\n",
    "\n",
    "        #testing\n",
    "\n",
    "        test_loss, acc = 0, 0\n",
    "\n",
    "    model_0.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "\n",
    "            #do forward pass\n",
    "\n",
    "        for batch, (X,y) in enumerate(test_data_batch):\n",
    "                #do the forward pass\n",
    "            y_test = model_0(X)\n",
    "\n",
    "                #calculate the loss\n",
    "            t_loss = loss_fn(y_test, y)\n",
    "            test_loss += t_loss\n",
    "\n",
    "        test_loss /= len(test_data_batch)\n",
    "\n",
    "    print(f\" the training loss is {train_loss:.6f}        <||||     test loss {test_loss:.6f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "\n",
    "print(my_run_time(start_time, end_time))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Non-Linearity to our Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTV1(nn.Module):\n",
    "\n",
    "    def __init__(self, input_shape: int, hidden_layers : int, output_shape: int ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_stacktwo = nn.Sequential( nn.Flatten(),\n",
    "                                             nn.Linear(in_features= input_shape, out_features= hidden_layers),\n",
    "                                             nn.ReLU(),\n",
    "                                             nn.Linear(in_features=hidden_layers, out_features= output_shape),\n",
    "                                             nn.ReLU())\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.linear_stacktwo(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTV1(\n",
       "  (linear_stacktwo): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (4): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instantiate the class\n",
    "\n",
    "model_1 = FashionMNISTV1(input_shape=784,\n",
    "                         hidden_layers= 10,\n",
    "                         output_shape=len(classnames)).to(device='cpu')\n",
    "\n",
    "model_1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and Accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import accuracy_fn\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model_1.parameters(), lr = 0.1)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functionalizing the testing and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model: torch.nn.Module,\n",
    "                  optimizer: torch.optim.Optimizer,\n",
    "                  loss: torch.nn.Module,\n",
    "                  dataloader: torch.utils.data.DataLoader,\n",
    "                  accuracy_fn,\n",
    "                  device: torch.device = device):\n",
    "    \n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    train_acc, train_loss = 0, 0\n",
    "\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        #do the forward pass\n",
    "\n",
    "        y_pred = model(X)\n",
    "\n",
    "        #calculate the loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss = loss\n",
    "\n",
    "        #accuracy function\n",
    "        train_acc = accuracy_fn(y_true= y, y_pred= y_pred.argmax(dim=1))\n",
    "\n",
    "        #optimizer\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        #optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 440 == 0:\n",
    "\n",
    "            print(f\"the train loss is {train_loss:.5f}     || \")\n",
    "            print(f\"the train accuracy is {train_acc:.5f}     || \")\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module,\n",
    "                  accuracy_fn,\n",
    "                  loss_fn: torch.nn.Module,\n",
    "                  dataloader: torch.utils.data.DataLoader,\n",
    "                  device: torch.device = device):\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    train_acc = 0\n",
    "\n",
    "\n",
    "    #loop through the dataset data\n",
    "\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        for batch, (X,y) in enumerate(dataloader):\n",
    "\n",
    "            #forwward pass\n",
    "\n",
    "            y_test = model(X)\n",
    "\n",
    "            #calculate the loss\n",
    "\n",
    "            loss = loss_fn(y_test, y)\n",
    "            test_loss = loss\n",
    "\n",
    "            train_acc = accuracy_fn(y_true= y, y_pred=y_test.argmax(dim=1)) \n",
    "\n",
    "            if batch % 440 == 0:\n",
    "                print(f\"the test loss is {test_loss:.5f}     || \")\n",
    "                print(f\" test accuracy is {train_acc}\")\n",
    "\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the train loss is 2.31190     || \n",
      "the train accuracy is 9.37500     || \n",
      "the train loss is 1.32879     || \n",
      "the train accuracy is 50.00000     || \n",
      "the train loss is 1.24919     || \n",
      "the train accuracy is 53.12500     || \n",
      "the train loss is 1.45807     || \n",
      "the train accuracy is 46.87500     || \n",
      "the train loss is 1.56323     || \n",
      "the train accuracy is 46.87500     || \n",
      "the test loss is 1.42059     || \n",
      " test accuracy is 53.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:08<00:16,  8.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the train loss is 0.82976     || \n",
      "the train accuracy is 68.75000     || \n",
      "the train loss is 1.15848     || \n",
      "the train accuracy is 53.12500     || \n",
      "the train loss is 1.13823     || \n",
      "the train accuracy is 56.25000     || \n",
      "the train loss is 0.60679     || \n",
      "the train accuracy is 78.12500     || \n",
      "the train loss is 1.25864     || \n",
      "the train accuracy is 50.00000     || \n",
      "the test loss is 1.23008     || \n",
      " test accuracy is 59.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:17<00:08,  8.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the train loss is 1.29489     || \n",
      "the train accuracy is 53.12500     || \n",
      "the train loss is 0.88828     || \n",
      "the train accuracy is 65.62500     || \n",
      "the train loss is 1.00067     || \n",
      "the train accuracy is 62.50000     || \n",
      "the train loss is 1.18874     || \n",
      "the train accuracy is 53.12500     || \n",
      "the train loss is 1.38647     || \n",
      "the train accuracy is 50.00000     || \n",
      "the test loss is 1.18750     || \n",
      " test accuracy is 62.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:27<00:00,  9.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the elapsed time is 27.246061325073242 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#run a loop through all the data \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    #training data\n",
    "    training_step(model=model_1,\n",
    "                  optimizer= optimizer,\n",
    "                  accuracy_fn= accuracy_fn,\n",
    "                  loss = loss_fn,\n",
    "                  dataloader=train_data_batch,\n",
    "                  device= 'cpu' )\n",
    "    \n",
    "    test_step(model= model_1,\n",
    "              loss_fn= loss_fn,\n",
    "              accuracy_fn=accuracy_fn,\n",
    "              dataloader= test_data_batch,\n",
    "              device= 'cpu')\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(my_run_time(start_time, end_time))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTV2(nn.Module):\n",
    "    #class constructor\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels= input_shape,\n",
    "                      out_channels= hidden_units,\n",
    "                      kernel_size= 3,\n",
    "                      stride= 1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d( in_channels = hidden_units,\n",
    "                      out_channels = hidden_units,\n",
    "                      kernel_size = 3,\n",
    "                      stride = 1,\n",
    "                      padding= 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size= 2, stride = 2))\n",
    "        \n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels= hidden_units,\n",
    "                      out_channels= hidden_units,\n",
    "                      kernel_size= 3,\n",
    "                      stride= 1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d( in_channels = hidden_units,\n",
    "                      out_channels = hidden_units,\n",
    "                      kernel_size = 3,\n",
    "                      stride = 1,\n",
    "                      padding= 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size= 2, stride = 2)            \n",
    "\n",
    "        )\n",
    "\n",
    "        dummy_input = torch.zeros((1, input_shape, 28, 28))\n",
    "        dummy_output = self.conv_block_2(self.conv_block_1(dummy_input))\n",
    "        input_size = dummy_output.flatten(1).shape[1]\n",
    "\n",
    "       \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features= input_size, out_features= output_shape, )\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "\n",
    "        x = self.conv_block_1(x)\n",
    "\n",
    "        x = self.conv_block_2(x)\n",
    "\n",
    "        x = x.flatten(1)\n",
    "\n",
    "       \n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTV2(\n",
       "  (conv_block_1): Sequential(\n",
       "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_block_2): Sequential(\n",
       "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=490, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create an instance of a class\n",
    "\n",
    "model_2 = FashionMNISTV2( input_shape=1,\n",
    "                         hidden_units= 10,\n",
    "                         output_shape= len(classnames))\n",
    "\n",
    "model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0164,  0.0423,  0.0199,  0.0422, -0.0517,  0.0184,  0.0162, -0.0477,\n",
       "         -0.0194, -0.0485]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pass random image through the model\n",
    "\n",
    "rand_out = model_2(img.unsqueeze(1))\n",
    "rand_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loss function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv_block_1.0.weight',\n",
       "              tensor([[[[ 0.1099,  0.0048, -0.1266],\n",
       "                        [-0.0932, -0.0460, -0.0676],\n",
       "                        [-0.2062,  0.2509,  0.0675]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2230, -0.0677, -0.2412],\n",
       "                        [-0.1948,  0.2532,  0.1018],\n",
       "                        [ 0.1460, -0.0432, -0.0832]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0431, -0.2368, -0.3317],\n",
       "                        [ 0.2343,  0.0068,  0.2022],\n",
       "                        [ 0.2508,  0.1458, -0.1139]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0611, -0.0174,  0.0199],\n",
       "                        [ 0.0130, -0.2606, -0.1832],\n",
       "                        [ 0.2429,  0.0107, -0.1511]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2254, -0.0989, -0.1356],\n",
       "                        [ 0.2330, -0.2630, -0.1596],\n",
       "                        [ 0.2523, -0.0326, -0.1375]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2766,  0.0507, -0.3132],\n",
       "                        [-0.2465, -0.0845, -0.0183],\n",
       "                        [-0.2510, -0.1256,  0.1127]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.3109, -0.3124,  0.0867],\n",
       "                        [ 0.0636, -0.3219, -0.3101],\n",
       "                        [ 0.2770,  0.3327, -0.1457]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3320,  0.2302,  0.2963],\n",
       "                        [-0.1434, -0.0816,  0.1965],\n",
       "                        [ 0.1236, -0.2660,  0.1995]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3251, -0.2117, -0.3095],\n",
       "                        [-0.3175, -0.2109, -0.1468],\n",
       "                        [-0.1884, -0.2919,  0.2082]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3260,  0.2791,  0.2087],\n",
       "                        [ 0.1480, -0.1314, -0.2690],\n",
       "                        [-0.1969,  0.1509,  0.0729]]]])),\n",
       "             ('conv_block_1.0.bias',\n",
       "              tensor([ 0.0426, -0.2023, -0.1568, -0.1406, -0.0423, -0.0629,  0.0353, -0.1110,\n",
       "                      -0.0378, -0.2120])),\n",
       "             ('conv_block_1.2.weight',\n",
       "              tensor([[[[ 0.0521, -0.0876,  0.0192],\n",
       "                        [-0.1003, -0.0601,  0.1019],\n",
       "                        [-0.0197, -0.0194,  0.0544]],\n",
       "              \n",
       "                       [[-0.0909, -0.0243, -0.0034],\n",
       "                        [ 0.0119, -0.0175, -0.0744],\n",
       "                        [-0.0966,  0.0011,  0.0880]],\n",
       "              \n",
       "                       [[-0.0473,  0.0133,  0.0670],\n",
       "                        [-0.1030,  0.0933,  0.0890],\n",
       "                        [-0.0461,  0.0919,  0.0720]],\n",
       "              \n",
       "                       [[ 0.0876,  0.0913,  0.0242],\n",
       "                        [ 0.0579, -0.0564,  0.0177],\n",
       "                        [ 0.1011, -0.0363,  0.0248]],\n",
       "              \n",
       "                       [[-0.0539,  0.0390,  0.0343],\n",
       "                        [ 0.0796,  0.0006,  0.0503],\n",
       "                        [ 0.0639,  0.0170, -0.0994]],\n",
       "              \n",
       "                       [[ 0.0747, -0.0675,  0.0488],\n",
       "                        [-0.0789,  0.0307, -0.0197],\n",
       "                        [-0.0944,  0.0481, -0.0245]],\n",
       "              \n",
       "                       [[-0.0439, -0.0328,  0.0717],\n",
       "                        [ 0.0619,  0.0287,  0.0441],\n",
       "                        [ 0.0541,  0.0436,  0.0898]],\n",
       "              \n",
       "                       [[ 0.0635, -0.0542, -0.0161],\n",
       "                        [ 0.0492,  0.0839, -0.0876],\n",
       "                        [-0.0523, -0.0523,  0.0996]],\n",
       "              \n",
       "                       [[ 0.1035, -0.0471, -0.0326],\n",
       "                        [ 0.0019,  0.0062, -0.0300],\n",
       "                        [-0.0769, -0.0561, -0.0088]],\n",
       "              \n",
       "                       [[ 0.0114,  0.0040, -0.0968],\n",
       "                        [ 0.0933, -0.0614, -0.0258],\n",
       "                        [ 0.0978,  0.0007, -0.0843]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0607, -0.0397,  0.1022],\n",
       "                        [-0.0306, -0.0788, -0.0784],\n",
       "                        [ 0.0006, -0.0170,  0.0821]],\n",
       "              \n",
       "                       [[ 0.0063, -0.0634,  0.0118],\n",
       "                        [-0.0780,  0.0157,  0.0278],\n",
       "                        [-0.0994, -0.0721, -0.0529]],\n",
       "              \n",
       "                       [[-0.0369, -0.0192,  0.0442],\n",
       "                        [-0.0832,  0.0545, -0.0731],\n",
       "                        [ 0.0926, -0.0602, -0.0806]],\n",
       "              \n",
       "                       [[ 0.0755,  0.0857, -0.0357],\n",
       "                        [ 0.0924,  0.0119,  0.0383],\n",
       "                        [-0.0062, -0.0288, -0.0381]],\n",
       "              \n",
       "                       [[-0.0904,  0.0769,  0.0558],\n",
       "                        [ 0.0958, -0.1036,  0.1041],\n",
       "                        [ 0.0703,  0.0287, -0.0452]],\n",
       "              \n",
       "                       [[ 0.0744,  0.0676,  0.0907],\n",
       "                        [-0.0870,  0.0160,  0.0380],\n",
       "                        [-0.1037,  0.0987,  0.0658]],\n",
       "              \n",
       "                       [[-0.0846, -0.0905,  0.0043],\n",
       "                        [ 0.0845,  0.0306,  0.0148],\n",
       "                        [-0.0068, -0.1047,  0.0634]],\n",
       "              \n",
       "                       [[ 0.0672, -0.0741, -0.0194],\n",
       "                        [ 0.1025, -0.0404, -0.0724],\n",
       "                        [ 0.0603,  0.0487,  0.0908]],\n",
       "              \n",
       "                       [[-0.0454,  0.0256,  0.0254],\n",
       "                        [ 0.0858,  0.0591,  0.0248],\n",
       "                        [-0.0148, -0.0188, -0.0569]],\n",
       "              \n",
       "                       [[ 0.0443,  0.0898, -0.0536],\n",
       "                        [-0.0720,  0.0785,  0.0142],\n",
       "                        [-0.0497,  0.0683, -0.0859]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0518, -0.0339, -0.0034],\n",
       "                        [ 0.0225,  0.0504, -0.0368],\n",
       "                        [-0.0102, -0.0550, -0.0693]],\n",
       "              \n",
       "                       [[ 0.0230,  0.0777,  0.0910],\n",
       "                        [-0.0823,  0.0499, -0.0325],\n",
       "                        [-0.0053, -0.0044,  0.0910]],\n",
       "              \n",
       "                       [[ 0.0883, -0.0854,  0.0229],\n",
       "                        [ 0.0616,  0.0283, -0.0024],\n",
       "                        [-0.0498, -0.0744, -0.1038]],\n",
       "              \n",
       "                       [[-0.0020, -0.0407,  0.0249],\n",
       "                        [-0.0446,  0.0876, -0.0744],\n",
       "                        [ 0.0935, -0.0418,  0.0163]],\n",
       "              \n",
       "                       [[-0.0387,  0.0444,  0.0832],\n",
       "                        [ 0.0986, -0.0730,  0.0724],\n",
       "                        [ 0.0697, -0.0865,  0.0426]],\n",
       "              \n",
       "                       [[ 0.0204,  0.0579,  0.0924],\n",
       "                        [ 0.0024, -0.0390, -0.0475],\n",
       "                        [ 0.0136, -0.0155, -0.0755]],\n",
       "              \n",
       "                       [[ 0.0029,  0.0604,  0.0035],\n",
       "                        [ 0.0663, -0.0398,  0.0827],\n",
       "                        [ 0.0251, -0.0354,  0.0508]],\n",
       "              \n",
       "                       [[ 0.0518, -0.0452, -0.0192],\n",
       "                        [ 0.0544, -0.0929,  0.0402],\n",
       "                        [ 0.0231, -0.0719, -0.0130]],\n",
       "              \n",
       "                       [[ 0.0309,  0.0733,  0.0771],\n",
       "                        [-0.0680, -0.0685,  0.0498],\n",
       "                        [ 0.1040,  0.0900,  0.0415]],\n",
       "              \n",
       "                       [[ 0.0999, -0.0484, -0.0030],\n",
       "                        [-0.0860, -0.0647, -0.0713],\n",
       "                        [-0.0956,  0.0297,  0.0272]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0424,  0.0699, -0.0840],\n",
       "                        [-0.0304,  0.0829, -0.0933],\n",
       "                        [ 0.0741,  0.0325, -0.0856]],\n",
       "              \n",
       "                       [[ 0.0227,  0.0034,  0.0204],\n",
       "                        [-0.0302,  0.0477,  0.0820],\n",
       "                        [ 0.0735, -0.0101, -0.0970]],\n",
       "              \n",
       "                       [[ 0.1032,  0.0293,  0.0936],\n",
       "                        [ 0.0380,  0.0017,  0.0567],\n",
       "                        [ 0.0590, -0.0068,  0.0161]],\n",
       "              \n",
       "                       [[-0.0043,  0.0722,  0.0480],\n",
       "                        [-0.0176,  0.0982,  0.1010],\n",
       "                        [ 0.0856,  0.0189,  0.0064]],\n",
       "              \n",
       "                       [[-0.0550,  0.0768, -0.0757],\n",
       "                        [-0.0909,  0.0318, -0.0986],\n",
       "                        [ 0.0336, -0.0508, -0.0013]],\n",
       "              \n",
       "                       [[-0.0406,  0.0852, -0.0783],\n",
       "                        [ 0.0194,  0.0015, -0.1003],\n",
       "                        [ 0.0815,  0.0208, -0.0846]],\n",
       "              \n",
       "                       [[-0.1046,  0.0466,  0.0865],\n",
       "                        [-0.0240, -0.0311, -0.0899],\n",
       "                        [ 0.0712, -0.0397, -0.0485]],\n",
       "              \n",
       "                       [[ 0.0401,  0.0922,  0.0488],\n",
       "                        [ 0.0304,  0.0933,  0.0045],\n",
       "                        [ 0.0163, -0.0077, -0.0660]],\n",
       "              \n",
       "                       [[ 0.0194, -0.0547,  0.0610],\n",
       "                        [-0.0727, -0.0862, -0.0204],\n",
       "                        [ 0.0783, -0.0801, -0.0336]],\n",
       "              \n",
       "                       [[-0.0081, -0.0461,  0.0498],\n",
       "                        [ 0.0921,  0.0571, -0.0222],\n",
       "                        [-0.0828, -0.0870, -0.0252]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0332, -0.0799,  0.0261],\n",
       "                        [-0.0399,  0.0306, -0.0560],\n",
       "                        [-0.0095, -0.0317,  0.0647]],\n",
       "              \n",
       "                       [[-0.0962, -0.0416,  0.0225],\n",
       "                        [-0.0297, -0.0018,  0.0712],\n",
       "                        [ 0.0151,  0.0130,  0.0308]],\n",
       "              \n",
       "                       [[-0.0567, -0.0421,  0.0505],\n",
       "                        [ 0.0153,  0.0688, -0.0135],\n",
       "                        [ 0.0990,  0.0630, -0.0295]],\n",
       "              \n",
       "                       [[-0.0539,  0.0213, -0.0879],\n",
       "                        [-0.0483,  0.0323,  0.0092],\n",
       "                        [ 0.0477, -0.0441,  0.0887]],\n",
       "              \n",
       "                       [[-0.0580, -0.0540,  0.0705],\n",
       "                        [-0.0203,  0.0302, -0.0187],\n",
       "                        [-0.0745, -0.0042,  0.0478]],\n",
       "              \n",
       "                       [[ 0.0710,  0.0804, -0.0037],\n",
       "                        [ 0.0763,  0.0297,  0.0039],\n",
       "                        [ 0.0020, -0.0101,  0.0301]],\n",
       "              \n",
       "                       [[-0.0054,  0.0678, -0.0161],\n",
       "                        [-0.0585,  0.0306, -0.0591],\n",
       "                        [ 0.0655,  0.0861,  0.0169]],\n",
       "              \n",
       "                       [[-0.0211, -0.0048,  0.0494],\n",
       "                        [-0.0805, -0.0282, -0.0642],\n",
       "                        [-0.0953, -0.0754, -0.0677]],\n",
       "              \n",
       "                       [[ 0.0803,  0.0644, -0.0716],\n",
       "                        [-0.0832,  0.0944,  0.0250],\n",
       "                        [ 0.0485,  0.0165, -0.0394]],\n",
       "              \n",
       "                       [[-0.0569,  0.0567, -0.1041],\n",
       "                        [-0.0877,  0.0318, -0.0276],\n",
       "                        [ 0.0202,  0.0562, -0.0606]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0593, -0.0648, -0.0922],\n",
       "                        [-0.0071,  0.0895, -0.0064],\n",
       "                        [ 0.0578, -0.0524,  0.0105]],\n",
       "              \n",
       "                       [[-0.0792, -0.0070,  0.0231],\n",
       "                        [-0.0451,  0.0968,  0.0298],\n",
       "                        [ 0.0527,  0.1049,  0.0447]],\n",
       "              \n",
       "                       [[-0.0083,  0.0503,  0.1039],\n",
       "                        [-0.1015, -0.0199,  0.0512],\n",
       "                        [-0.0255, -0.0858, -0.0612]],\n",
       "              \n",
       "                       [[-0.0548,  0.0437,  0.0671],\n",
       "                        [-0.0120, -0.0863,  0.0150],\n",
       "                        [ 0.0295, -0.0398,  0.0363]],\n",
       "              \n",
       "                       [[-0.0431,  0.0355,  0.0754],\n",
       "                        [ 0.0526, -0.0100,  0.1012],\n",
       "                        [-0.0093, -0.0940,  0.0758]],\n",
       "              \n",
       "                       [[ 0.0980,  0.0481,  0.0924],\n",
       "                        [ 0.0569,  0.0430,  0.0610],\n",
       "                        [-0.0810, -0.0013, -0.0450]],\n",
       "              \n",
       "                       [[ 0.0290, -0.0932,  0.0062],\n",
       "                        [ 0.0412, -0.0392, -0.0425],\n",
       "                        [-0.0684,  0.0290, -0.0885]],\n",
       "              \n",
       "                       [[-0.0216,  0.0059,  0.0301],\n",
       "                        [-0.0905,  0.0213, -0.0414],\n",
       "                        [-0.0123, -0.0769, -0.0828]],\n",
       "              \n",
       "                       [[ 0.0144,  0.0179,  0.0906],\n",
       "                        [ 0.0992,  0.0770,  0.0408],\n",
       "                        [ 0.0701,  0.0450, -0.0982]],\n",
       "              \n",
       "                       [[ 0.0767,  0.0128, -0.0263],\n",
       "                        [-0.0313,  0.0841,  0.0736],\n",
       "                        [-0.0098, -0.0394, -0.0567]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0608, -0.0639, -0.0699],\n",
       "                        [-0.0633,  0.0306,  0.0808],\n",
       "                        [ 0.0211,  0.0393, -0.0851]],\n",
       "              \n",
       "                       [[-0.0766, -0.0617, -0.0675],\n",
       "                        [-0.0471, -0.0639, -0.0345],\n",
       "                        [-0.0653,  0.0314,  0.0027]],\n",
       "              \n",
       "                       [[ 0.0054,  0.0447, -0.0949],\n",
       "                        [-0.0413, -0.0036, -0.0985],\n",
       "                        [ 0.0560, -0.0792,  0.0713]],\n",
       "              \n",
       "                       [[ 0.1043,  0.0217, -0.0513],\n",
       "                        [ 0.0744, -0.0691,  0.0050],\n",
       "                        [ 0.0661,  0.0988,  0.0077]],\n",
       "              \n",
       "                       [[-0.0080, -0.0091, -0.1022],\n",
       "                        [ 0.0187, -0.0837,  0.0095],\n",
       "                        [-0.0250,  0.0670, -0.0422]],\n",
       "              \n",
       "                       [[ 0.0749, -0.0251, -0.0204],\n",
       "                        [-0.0960,  0.0916,  0.0091],\n",
       "                        [-0.0950,  0.0059,  0.0432]],\n",
       "              \n",
       "                       [[-0.0223,  0.0860,  0.0516],\n",
       "                        [-0.0135,  0.1054,  0.0743],\n",
       "                        [-0.0129, -0.0091,  0.0324]],\n",
       "              \n",
       "                       [[-0.0367,  0.0649, -0.1014],\n",
       "                        [-0.0565,  0.0483, -0.1012],\n",
       "                        [-0.0349, -0.0560,  0.0571]],\n",
       "              \n",
       "                       [[ 0.0053, -0.0749,  0.0849],\n",
       "                        [-0.0481, -0.0116,  0.0356],\n",
       "                        [-0.0703, -0.0097, -0.0355]],\n",
       "              \n",
       "                       [[-0.0250, -0.0382,  0.0675],\n",
       "                        [-0.0518, -0.0711,  0.0455],\n",
       "                        [ 0.0342,  0.0011,  0.0087]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1035, -0.0353, -0.0941],\n",
       "                        [-0.1038,  0.0273,  0.0411],\n",
       "                        [ 0.0015, -0.0876, -0.0823]],\n",
       "              \n",
       "                       [[-0.0718,  0.0679,  0.0207],\n",
       "                        [-0.0587, -0.0997,  0.0606],\n",
       "                        [ 0.0011,  0.1048, -0.0864]],\n",
       "              \n",
       "                       [[-0.0855,  0.0182,  0.0706],\n",
       "                        [ 0.0696,  0.1048,  0.0671],\n",
       "                        [ 0.0471, -0.0648,  0.1039]],\n",
       "              \n",
       "                       [[-0.0467,  0.0678,  0.0434],\n",
       "                        [-0.0625, -0.0094, -0.0088],\n",
       "                        [ 0.0796,  0.0807,  0.0226]],\n",
       "              \n",
       "                       [[-0.0599,  0.0300,  0.0864],\n",
       "                        [-0.0559,  0.0845,  0.0238],\n",
       "                        [ 0.0583,  0.0362,  0.0665]],\n",
       "              \n",
       "                       [[-0.0897,  0.0435, -0.0914],\n",
       "                        [-0.0072,  0.0824, -0.0864],\n",
       "                        [-0.0802, -0.0462,  0.0040]],\n",
       "              \n",
       "                       [[-0.0146, -0.0827,  0.0137],\n",
       "                        [-0.0265, -0.0492,  0.0012],\n",
       "                        [ 0.0358,  0.0169, -0.0699]],\n",
       "              \n",
       "                       [[ 0.0715,  0.0199, -0.0432],\n",
       "                        [ 0.0912,  0.0212,  0.0548],\n",
       "                        [ 0.0745, -0.0019,  0.0869]],\n",
       "              \n",
       "                       [[-0.0005,  0.0843,  0.0128],\n",
       "                        [ 0.0485,  0.0531, -0.0584],\n",
       "                        [ 0.0921,  0.0032,  0.0387]],\n",
       "              \n",
       "                       [[-0.0778, -0.0393,  0.0687],\n",
       "                        [-0.0665,  0.0146,  0.0926],\n",
       "                        [-0.1001, -0.0012,  0.0171]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0691, -0.0354, -0.1030],\n",
       "                        [-0.0652,  0.0316, -0.0776],\n",
       "                        [-0.0902, -0.0027,  0.0277]],\n",
       "              \n",
       "                       [[-0.0661, -0.0409,  0.0577],\n",
       "                        [-0.0713, -0.0867, -0.0319],\n",
       "                        [ 0.0847, -0.0309, -0.0715]],\n",
       "              \n",
       "                       [[-0.0748, -0.0248,  0.0749],\n",
       "                        [ 0.0396, -0.0352, -0.0914],\n",
       "                        [-0.0640,  0.0223,  0.0886]],\n",
       "              \n",
       "                       [[ 0.0543, -0.0135,  0.0559],\n",
       "                        [ 0.0076,  0.0145,  0.0491],\n",
       "                        [-0.0259,  0.0307,  0.0094]],\n",
       "              \n",
       "                       [[ 0.0322, -0.0225,  0.0681],\n",
       "                        [-0.0950,  0.0700, -0.0870],\n",
       "                        [-0.0709, -0.0648, -0.0102]],\n",
       "              \n",
       "                       [[-0.0104, -0.1025,  0.0559],\n",
       "                        [ 0.0259,  0.1030, -0.0752],\n",
       "                        [-0.0848,  0.0253,  0.0812]],\n",
       "              \n",
       "                       [[ 0.0704,  0.0428,  0.0177],\n",
       "                        [ 0.0525, -0.0420, -0.1051],\n",
       "                        [-0.0878, -0.0674,  0.0235]],\n",
       "              \n",
       "                       [[ 0.0944,  0.0680, -0.0688],\n",
       "                        [ 0.0294,  0.0932,  0.0026],\n",
       "                        [-0.0196, -0.0571, -0.0077]],\n",
       "              \n",
       "                       [[ 0.0793, -0.0298, -0.0159],\n",
       "                        [ 0.0511, -0.0404, -0.0487],\n",
       "                        [ 0.0178,  0.0763,  0.0112]],\n",
       "              \n",
       "                       [[-0.0042, -0.0837,  0.0629],\n",
       "                        [ 0.0048, -0.0464, -0.0927],\n",
       "                        [-0.0500, -0.0741,  0.0216]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0772, -0.0208,  0.0642],\n",
       "                        [-0.0224, -0.0502, -0.0965],\n",
       "                        [-0.0706,  0.0791,  0.0710]],\n",
       "              \n",
       "                       [[ 0.1017, -0.0106,  0.0356],\n",
       "                        [-0.0129,  0.1043, -0.0841],\n",
       "                        [-0.0286, -0.1011,  0.0104]],\n",
       "              \n",
       "                       [[-0.0821, -0.0739,  0.0244],\n",
       "                        [ 0.1049,  0.0879,  0.0724],\n",
       "                        [-0.0261, -0.0032, -0.0948]],\n",
       "              \n",
       "                       [[ 0.0709,  0.0288,  0.0895],\n",
       "                        [-0.1025, -0.0257, -0.0679],\n",
       "                        [-0.0807, -0.0627,  0.0333]],\n",
       "              \n",
       "                       [[ 0.0701,  0.0476, -0.0361],\n",
       "                        [ 0.0465,  0.1038,  0.0052],\n",
       "                        [-0.0379, -0.0832,  0.0745]],\n",
       "              \n",
       "                       [[-0.0241, -0.0827,  0.0610],\n",
       "                        [ 0.0210, -0.0425, -0.1038],\n",
       "                        [ 0.0132, -0.0312, -0.0183]],\n",
       "              \n",
       "                       [[-0.0352,  0.0867,  0.0560],\n",
       "                        [ 0.0836,  0.0398,  0.0301],\n",
       "                        [ 0.1040, -0.0004,  0.0606]],\n",
       "              \n",
       "                       [[ 0.0429, -0.1009,  0.0716],\n",
       "                        [-0.0103, -0.0712,  0.0710],\n",
       "                        [-0.0854,  0.0494, -0.0867]],\n",
       "              \n",
       "                       [[-0.0302, -0.0989,  0.0754],\n",
       "                        [ 0.0261,  0.0860,  0.0322],\n",
       "                        [-0.0523,  0.0159,  0.0713]],\n",
       "              \n",
       "                       [[-0.0815, -0.0703, -0.0268],\n",
       "                        [-0.0659, -0.0191,  0.0998],\n",
       "                        [ 0.0586, -0.0855, -0.0569]]]])),\n",
       "             ('conv_block_1.2.bias',\n",
       "              tensor([ 0.0846, -0.0081, -0.0444,  0.0206, -0.0167,  0.0355,  0.0019,  0.0599,\n",
       "                       0.0580,  0.0370])),\n",
       "             ('conv_block_2.0.weight',\n",
       "              tensor([[[[ 3.2651e-02, -2.7069e-02, -9.7559e-02],\n",
       "                        [-1.0148e-02, -8.0057e-02, -8.4320e-02],\n",
       "                        [-9.2818e-02, -2.6341e-02, -7.7951e-02]],\n",
       "              \n",
       "                       [[ 1.7822e-02,  4.5520e-02,  6.2623e-02],\n",
       "                        [-1.0174e-01,  8.8691e-02,  3.1364e-02],\n",
       "                        [ 8.5540e-02,  5.1075e-02,  1.9162e-02]],\n",
       "              \n",
       "                       [[-2.0232e-02,  8.8187e-02, -6.8607e-02],\n",
       "                        [ 8.2221e-02,  9.1940e-02,  1.7215e-02],\n",
       "                        [-4.5205e-02, -8.5152e-02, -5.7938e-03]],\n",
       "              \n",
       "                       [[ 8.9570e-02,  4.8727e-03, -2.6532e-02],\n",
       "                        [-5.2422e-02,  7.8767e-03,  5.8294e-02],\n",
       "                        [ 2.0535e-02,  1.8175e-02, -7.2943e-02]],\n",
       "              \n",
       "                       [[ 5.1012e-02,  1.0956e-02, -1.0360e-01],\n",
       "                        [ 8.7677e-02,  6.7035e-02,  8.2627e-04],\n",
       "                        [-2.0821e-02,  9.4677e-02,  1.0465e-01]],\n",
       "              \n",
       "                       [[ 1.7178e-02,  9.7850e-02,  4.4562e-02],\n",
       "                        [ 5.0305e-02, -6.2430e-03, -7.6995e-03],\n",
       "                        [ 6.7413e-02, -4.6169e-02, -7.5294e-02]],\n",
       "              \n",
       "                       [[ 6.5815e-02, -2.5299e-02,  7.4421e-02],\n",
       "                        [-1.7514e-02, -6.4215e-02, -1.7835e-03],\n",
       "                        [-1.2153e-02,  2.4195e-02, -8.1323e-02]],\n",
       "              \n",
       "                       [[-8.9247e-02,  5.4510e-03,  9.5141e-02],\n",
       "                        [-6.6632e-02, -5.3025e-02,  1.0060e-02],\n",
       "                        [ 7.8939e-02,  8.0782e-02, -2.2010e-02]],\n",
       "              \n",
       "                       [[-5.5260e-03, -8.2991e-02,  1.0295e-01],\n",
       "                        [ 6.4041e-02,  2.6951e-02, -1.0240e-01],\n",
       "                        [ 5.9585e-02, -5.9392e-02,  4.3869e-03]],\n",
       "              \n",
       "                       [[ 5.0576e-02, -4.1981e-03, -5.1413e-02],\n",
       "                        [ 7.1199e-02,  7.1445e-02,  1.0268e-02],\n",
       "                        [-6.5874e-02, -5.4607e-02, -9.4689e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.6417e-02, -9.4155e-02, -9.3014e-03],\n",
       "                        [-7.4860e-02,  6.0031e-02, -8.5295e-02],\n",
       "                        [-7.2671e-02, -6.4953e-02, -2.7696e-02]],\n",
       "              \n",
       "                       [[ 4.1251e-02,  7.5863e-02,  1.0455e-01],\n",
       "                        [-2.5140e-02,  2.4747e-02,  7.4215e-03],\n",
       "                        [ 5.8136e-02,  4.9535e-02,  6.1977e-02]],\n",
       "              \n",
       "                       [[ 7.1899e-02,  8.7750e-02,  5.3851e-03],\n",
       "                        [ 7.3791e-02, -5.9229e-02, -2.2137e-02],\n",
       "                        [-2.9449e-02, -3.1561e-02,  6.0360e-03]],\n",
       "              \n",
       "                       [[-9.9174e-02, -5.9743e-02, -9.8813e-03],\n",
       "                        [ 2.1199e-02,  7.4684e-02, -7.2374e-02],\n",
       "                        [ 2.5719e-02,  4.8645e-02, -5.6244e-02]],\n",
       "              \n",
       "                       [[ 1.7926e-02, -5.5249e-02, -2.8103e-03],\n",
       "                        [-5.7923e-02, -2.9498e-02, -4.1699e-02],\n",
       "                        [ 9.1202e-02,  2.5441e-02, -1.0226e-01]],\n",
       "              \n",
       "                       [[ 1.3339e-02,  5.7482e-02, -4.5904e-02],\n",
       "                        [-6.3036e-02, -2.0697e-02, -9.3108e-02],\n",
       "                        [-3.9219e-02,  1.2948e-02, -7.8643e-02]],\n",
       "              \n",
       "                       [[-1.0012e-01, -6.2667e-02, -7.7001e-02],\n",
       "                        [ 3.0953e-03,  3.8097e-02,  6.6416e-02],\n",
       "                        [ 1.0533e-01,  5.5096e-02, -4.7590e-02]],\n",
       "              \n",
       "                       [[-1.4002e-02, -5.4131e-02,  6.3354e-02],\n",
       "                        [ 3.5143e-02,  4.8587e-02, -7.0820e-02],\n",
       "                        [ 8.2173e-02, -9.2799e-02,  5.2920e-02]],\n",
       "              \n",
       "                       [[-9.1949e-02,  8.0714e-02, -6.2491e-02],\n",
       "                        [-8.2032e-02,  5.2740e-02,  9.3458e-02],\n",
       "                        [-7.6663e-03,  4.4115e-02,  9.6442e-02]],\n",
       "              \n",
       "                       [[-8.5662e-02,  4.2861e-02,  2.4263e-02],\n",
       "                        [-5.9977e-02,  6.2596e-02,  9.3894e-02],\n",
       "                        [ 8.1056e-02, -7.5743e-02,  9.3207e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 8.8665e-02,  4.0083e-02, -1.4455e-02],\n",
       "                        [ 8.1110e-02, -6.8314e-02, -6.4043e-02],\n",
       "                        [ 5.7970e-02,  7.0311e-02, -1.0642e-02]],\n",
       "              \n",
       "                       [[ 9.6939e-02,  5.5507e-02,  5.1559e-02],\n",
       "                        [ 8.5458e-02,  1.0377e-01,  7.3722e-02],\n",
       "                        [ 8.5893e-02,  9.9354e-03,  8.2613e-02]],\n",
       "              \n",
       "                       [[ 5.6645e-02, -5.1838e-02,  5.8911e-02],\n",
       "                        [ 7.3474e-03, -3.6386e-02,  7.9076e-02],\n",
       "                        [ 2.7574e-02,  8.2139e-02,  2.2726e-02]],\n",
       "              \n",
       "                       [[ 3.1477e-02,  6.3293e-02, -2.6282e-02],\n",
       "                        [ 6.5506e-02, -5.4486e-02, -1.4932e-02],\n",
       "                        [ 9.6702e-03,  7.2861e-02,  4.6455e-02]],\n",
       "              \n",
       "                       [[ 1.0170e-01, -5.6602e-02, -8.3367e-03],\n",
       "                        [-8.7885e-03,  8.5905e-02,  9.9512e-02],\n",
       "                        [ 2.6604e-02, -8.5186e-02, -7.6461e-02]],\n",
       "              \n",
       "                       [[ 1.9347e-02, -9.2857e-02, -3.6767e-02],\n",
       "                        [-7.9037e-02,  5.4059e-02, -1.4467e-02],\n",
       "                        [-8.2529e-02,  6.8878e-02, -9.3455e-02]],\n",
       "              \n",
       "                       [[-1.0305e-02,  1.0149e-01,  5.2840e-02],\n",
       "                        [-9.1326e-02,  1.1373e-02, -1.5692e-02],\n",
       "                        [ 4.6939e-02,  9.7392e-02,  8.9842e-02]],\n",
       "              \n",
       "                       [[-1.0061e-02,  4.8565e-02, -1.0352e-01],\n",
       "                        [ 8.3045e-02,  5.6017e-02, -7.4144e-02],\n",
       "                        [-3.0487e-02, -4.0981e-02, -6.1630e-02]],\n",
       "              \n",
       "                       [[ 4.3434e-02, -3.0735e-02,  4.3840e-02],\n",
       "                        [-3.2028e-02,  8.1881e-02, -4.1468e-02],\n",
       "                        [-1.8260e-02, -3.2772e-02,  4.8531e-03]],\n",
       "              \n",
       "                       [[-3.8844e-02, -7.4038e-02, -1.0017e-01],\n",
       "                        [ 6.5567e-02,  1.4626e-02, -6.7759e-02],\n",
       "                        [ 5.4616e-02,  1.1030e-02, -3.8064e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.9070e-03,  1.1591e-02,  7.5523e-02],\n",
       "                        [-9.6889e-02, -8.1814e-02, -8.4987e-02],\n",
       "                        [-8.2766e-02,  6.6534e-04,  5.8534e-02]],\n",
       "              \n",
       "                       [[ 2.7553e-02,  4.4661e-02,  1.2682e-02],\n",
       "                        [-2.0089e-03, -9.5493e-02,  9.4088e-02],\n",
       "                        [ 1.0187e-01, -6.0086e-04,  1.9044e-02]],\n",
       "              \n",
       "                       [[ 1.9516e-02,  1.0432e-01, -2.0113e-04],\n",
       "                        [-1.7340e-02, -5.7392e-02,  2.7256e-02],\n",
       "                        [ 4.8595e-02, -8.8971e-02,  3.6058e-02]],\n",
       "              \n",
       "                       [[ 2.6327e-02,  4.6756e-02,  4.9416e-02],\n",
       "                        [-2.7598e-02,  1.4115e-02, -7.3856e-02],\n",
       "                        [ 6.3450e-02, -8.8688e-02, -9.2476e-02]],\n",
       "              \n",
       "                       [[-7.8171e-02, -5.8813e-02,  4.3989e-03],\n",
       "                        [ 6.9174e-02, -1.3888e-02,  7.5693e-02],\n",
       "                        [-1.7223e-02,  8.9379e-02,  7.2752e-02]],\n",
       "              \n",
       "                       [[ 6.6572e-02,  4.3033e-02, -7.0999e-02],\n",
       "                        [ 1.0330e-01, -1.0032e-01, -7.3892e-02],\n",
       "                        [-6.3699e-02, -2.7816e-02,  4.5782e-02]],\n",
       "              \n",
       "                       [[ 2.1828e-02, -9.0719e-02,  1.0177e-01],\n",
       "                        [ 9.8932e-02,  5.1986e-02, -1.0326e-01],\n",
       "                        [ 5.8887e-02,  3.0076e-02, -1.3031e-02]],\n",
       "              \n",
       "                       [[-8.8305e-02,  8.9305e-02, -3.1511e-02],\n",
       "                        [-8.5262e-02,  8.1359e-02,  4.1137e-02],\n",
       "                        [ 3.1031e-02,  6.8128e-02,  3.6721e-02]],\n",
       "              \n",
       "                       [[-2.2942e-02, -1.8330e-02,  8.7147e-02],\n",
       "                        [-3.4551e-03, -8.8047e-02, -7.2841e-02],\n",
       "                        [-7.9551e-02,  1.0075e-02,  7.7079e-02]],\n",
       "              \n",
       "                       [[ 6.3057e-02,  2.7881e-02,  3.5131e-02],\n",
       "                        [-4.9928e-02,  9.8661e-02,  4.0202e-02],\n",
       "                        [-6.6438e-02, -7.0223e-02, -9.4434e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.9989e-02,  7.7099e-02,  6.1887e-03],\n",
       "                        [-4.9958e-02,  2.9342e-02, -7.3168e-02],\n",
       "                        [ 3.1421e-02,  7.1970e-02,  5.1253e-02]],\n",
       "              \n",
       "                       [[-3.2093e-02, -3.5865e-02,  4.3939e-02],\n",
       "                        [ 6.3114e-02, -2.5831e-02, -2.3889e-04],\n",
       "                        [-1.1888e-02, -1.2388e-02, -9.3541e-02]],\n",
       "              \n",
       "                       [[-8.6972e-02,  9.0372e-02, -5.0760e-02],\n",
       "                        [ 7.7939e-02,  6.5398e-02, -1.6033e-02],\n",
       "                        [-5.0987e-02,  3.4998e-03, -1.8273e-02]],\n",
       "              \n",
       "                       [[-6.2539e-02,  1.0246e-01, -8.3265e-03],\n",
       "                        [-1.6206e-02, -7.1414e-02,  1.7144e-02],\n",
       "                        [ 1.4609e-02,  1.0395e-01, -3.2550e-02]],\n",
       "              \n",
       "                       [[ 5.9282e-02,  5.1128e-03,  5.5959e-02],\n",
       "                        [ 1.8476e-03, -1.8174e-02,  4.1318e-02],\n",
       "                        [-4.6952e-02,  6.3878e-02, -8.8288e-02]],\n",
       "              \n",
       "                       [[-7.5564e-02,  1.0043e-01, -3.9721e-02],\n",
       "                        [-8.3814e-02, -7.7056e-02,  1.0222e-01],\n",
       "                        [-1.4742e-02, -8.8699e-02, -1.5195e-02]],\n",
       "              \n",
       "                       [[-5.1384e-02, -7.0707e-02, -5.1582e-02],\n",
       "                        [ 7.9840e-02,  4.2519e-02,  6.1589e-02],\n",
       "                        [ 8.6999e-03, -4.4069e-02, -4.3021e-02]],\n",
       "              \n",
       "                       [[-7.0746e-02, -7.6156e-02, -9.0884e-02],\n",
       "                        [ 1.8727e-02,  5.5311e-02,  8.6018e-02],\n",
       "                        [-2.6769e-03, -1.5360e-02, -8.7618e-03]],\n",
       "              \n",
       "                       [[ 4.1531e-02, -1.6696e-02, -3.7038e-02],\n",
       "                        [-2.9044e-02, -1.0297e-01, -8.1178e-02],\n",
       "                        [-4.5120e-02,  1.0213e-02, -6.6206e-02]],\n",
       "              \n",
       "                       [[ 4.0370e-02, -5.7650e-02,  1.0198e-01],\n",
       "                        [-9.3156e-02,  5.8776e-02, -5.4730e-02],\n",
       "                        [-3.7375e-02, -9.8425e-02, -9.7693e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-9.9757e-02, -2.0246e-03,  9.7090e-02],\n",
       "                        [ 4.8332e-02,  6.6002e-02,  2.3602e-02],\n",
       "                        [-4.0727e-02, -4.1052e-02, -3.1312e-02]],\n",
       "              \n",
       "                       [[-9.8483e-02,  5.7941e-02, -3.1334e-02],\n",
       "                        [-5.9626e-02,  2.6153e-02,  2.7016e-06],\n",
       "                        [ 5.7326e-02, -6.9030e-02,  1.0472e-01]],\n",
       "              \n",
       "                       [[-4.1959e-02,  3.9828e-02,  9.5960e-02],\n",
       "                        [-6.0469e-03, -9.2625e-02, -1.0198e-01],\n",
       "                        [ 1.0369e-01, -9.8691e-02, -3.9103e-02]],\n",
       "              \n",
       "                       [[ 7.1895e-02, -8.1988e-02,  9.8508e-02],\n",
       "                        [ 1.0237e-02,  3.2764e-02,  1.6540e-02],\n",
       "                        [ 1.1591e-02,  5.9159e-02, -9.1334e-02]],\n",
       "              \n",
       "                       [[ 2.2786e-02, -4.7706e-02,  9.9761e-02],\n",
       "                        [ 8.7274e-02, -4.1407e-02,  5.8616e-02],\n",
       "                        [-1.7950e-02,  5.5621e-02, -2.9814e-02]],\n",
       "              \n",
       "                       [[-6.1685e-02,  4.4472e-02,  5.2192e-02],\n",
       "                        [ 9.4913e-02,  3.3406e-02, -4.7038e-02],\n",
       "                        [ 2.6931e-02,  6.3446e-03,  9.1701e-02]],\n",
       "              \n",
       "                       [[ 4.3246e-02,  8.9906e-02,  7.8296e-03],\n",
       "                        [-7.9909e-02, -6.7493e-02,  9.5178e-02],\n",
       "                        [ 9.4405e-02,  3.1753e-02, -7.5225e-02]],\n",
       "              \n",
       "                       [[ 7.7025e-02,  4.3726e-02, -8.3173e-02],\n",
       "                        [ 3.0645e-02,  1.0160e-01, -6.9621e-02],\n",
       "                        [ 8.0108e-03,  3.0115e-02, -3.7939e-02]],\n",
       "              \n",
       "                       [[ 6.9383e-02,  9.3739e-02,  1.0261e-01],\n",
       "                        [-9.4346e-02,  5.8681e-03,  8.6042e-02],\n",
       "                        [ 8.9793e-02, -2.7153e-02,  6.9830e-02]],\n",
       "              \n",
       "                       [[ 7.8677e-02, -6.6675e-03,  7.2295e-02],\n",
       "                        [ 6.0602e-02, -1.5680e-02, -4.4034e-02],\n",
       "                        [-5.5905e-02, -8.4932e-02, -6.3003e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.7519e-02,  3.7614e-02, -3.0224e-02],\n",
       "                        [-8.7174e-03,  7.2957e-02,  1.5613e-02],\n",
       "                        [-7.3139e-02,  8.9310e-02,  6.3604e-02]],\n",
       "              \n",
       "                       [[ 1.2143e-02,  5.0237e-02, -8.9102e-02],\n",
       "                        [ 6.1821e-02,  2.5649e-02, -9.3234e-03],\n",
       "                        [-3.3816e-02,  8.5131e-02,  2.2710e-02]],\n",
       "              \n",
       "                       [[-5.8883e-02,  4.7399e-02,  1.8644e-02],\n",
       "                        [-3.6957e-02,  7.4432e-02,  1.0495e-01],\n",
       "                        [-4.6544e-03, -4.3479e-02, -5.6571e-02]],\n",
       "              \n",
       "                       [[-5.5015e-02,  5.4084e-02,  6.6114e-02],\n",
       "                        [-6.3881e-02,  5.8385e-02,  4.2001e-03],\n",
       "                        [-3.6731e-03, -3.3525e-02, -9.7098e-02]],\n",
       "              \n",
       "                       [[-9.0412e-02,  7.7296e-02, -1.7267e-02],\n",
       "                        [-4.6015e-03, -6.3568e-02, -1.9248e-02],\n",
       "                        [-4.1121e-02,  9.4869e-03, -1.0062e-01]],\n",
       "              \n",
       "                       [[ 1.0223e-02, -9.3302e-02, -7.4924e-02],\n",
       "                        [-2.2504e-03, -1.1228e-02, -2.9630e-02],\n",
       "                        [ 6.1299e-02,  7.6117e-03,  2.1847e-02]],\n",
       "              \n",
       "                       [[ 4.3130e-02,  5.9518e-03, -9.3366e-02],\n",
       "                        [-3.9201e-02,  6.6187e-02,  6.0465e-02],\n",
       "                        [ 3.6464e-02,  9.3042e-02, -5.4805e-02]],\n",
       "              \n",
       "                       [[-3.1411e-02, -1.5705e-02, -2.5906e-02],\n",
       "                        [ 9.0919e-02, -1.0533e-01,  1.9972e-02],\n",
       "                        [ 1.0346e-01, -5.8595e-02,  8.3776e-02]],\n",
       "              \n",
       "                       [[-9.0819e-02,  1.7828e-02, -6.0007e-02],\n",
       "                        [ 3.3350e-02,  2.3059e-02,  2.3175e-02],\n",
       "                        [-2.3912e-02, -3.2651e-02, -1.0208e-01]],\n",
       "              \n",
       "                       [[ 1.7486e-02,  1.0062e-01, -7.9319e-02],\n",
       "                        [-4.0551e-02,  2.7661e-02,  5.3916e-02],\n",
       "                        [ 3.8284e-02,  6.8298e-02,  1.3326e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.5934e-02, -8.7866e-02, -1.0536e-01],\n",
       "                        [-8.0721e-02,  5.2953e-02,  9.4731e-02],\n",
       "                        [ 1.0034e-01,  7.8943e-02,  5.4402e-03]],\n",
       "              \n",
       "                       [[ 7.2584e-02,  4.0075e-02, -7.1373e-02],\n",
       "                        [-1.9783e-02, -9.7830e-02,  9.0487e-02],\n",
       "                        [ 2.2911e-02,  9.6796e-02, -1.2764e-02]],\n",
       "              \n",
       "                       [[ 8.3702e-02,  4.9339e-02, -1.2866e-02],\n",
       "                        [-3.4860e-03,  7.5126e-02, -1.3863e-02],\n",
       "                        [ 8.1963e-02,  2.3124e-02, -4.1294e-02]],\n",
       "              \n",
       "                       [[-4.8859e-02, -5.1272e-02,  6.7632e-02],\n",
       "                        [ 7.9680e-02, -4.9556e-02, -9.6216e-02],\n",
       "                        [ 5.9744e-02, -2.3969e-02, -9.6786e-02]],\n",
       "              \n",
       "                       [[ 2.2130e-02, -1.0488e-02,  7.5944e-02],\n",
       "                        [ 2.2378e-02, -6.4989e-02, -1.9063e-02],\n",
       "                        [-7.5693e-02,  4.9288e-02,  3.0835e-02]],\n",
       "              \n",
       "                       [[ 6.2224e-02, -7.8014e-02, -5.8675e-02],\n",
       "                        [-6.4147e-02, -8.7407e-02, -9.8320e-02],\n",
       "                        [-8.5021e-02,  7.3429e-02, -3.1801e-02]],\n",
       "              \n",
       "                       [[-1.9447e-02,  3.1772e-02,  2.7744e-02],\n",
       "                        [ 4.7391e-02, -7.4427e-02,  9.3663e-02],\n",
       "                        [-8.4210e-03, -5.0056e-02,  6.8037e-02]],\n",
       "              \n",
       "                       [[-8.7642e-02, -6.2837e-02,  5.9406e-02],\n",
       "                        [ 4.0862e-02, -4.1745e-02, -8.4239e-03],\n",
       "                        [-1.9251e-02,  4.2863e-02,  2.1148e-02]],\n",
       "              \n",
       "                       [[ 3.8629e-02,  8.7068e-02, -7.2497e-02],\n",
       "                        [-8.1085e-02, -2.5007e-02, -4.8000e-02],\n",
       "                        [-7.4963e-02,  9.7826e-02,  8.8060e-02]],\n",
       "              \n",
       "                       [[ 8.5112e-02, -6.4922e-02, -1.2465e-02],\n",
       "                        [-8.2016e-02, -3.8007e-02,  8.7840e-02],\n",
       "                        [ 1.3215e-02,  3.6567e-02, -1.0478e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.3427e-02,  7.1074e-02, -4.0477e-02],\n",
       "                        [ 8.0101e-02,  4.7477e-02, -7.6966e-02],\n",
       "                        [-7.9017e-02,  1.4799e-03, -1.0167e-01]],\n",
       "              \n",
       "                       [[-7.3664e-02,  4.1230e-02, -3.4682e-02],\n",
       "                        [-3.7351e-03, -4.3068e-02,  6.8224e-02],\n",
       "                        [ 8.6558e-02,  7.6061e-02, -9.3737e-02]],\n",
       "              \n",
       "                       [[-3.1164e-02,  5.1053e-02,  6.2345e-02],\n",
       "                        [-2.3625e-02,  5.7719e-02, -3.1165e-03],\n",
       "                        [ 1.1708e-02,  7.1024e-02,  7.2109e-02]],\n",
       "              \n",
       "                       [[-7.4416e-02, -6.3995e-02,  3.7341e-02],\n",
       "                        [ 6.3353e-02, -2.4544e-02,  9.3477e-02],\n",
       "                        [-1.2143e-02, -5.3455e-02, -1.0760e-02]],\n",
       "              \n",
       "                       [[-8.9718e-02,  3.2660e-02, -9.6911e-02],\n",
       "                        [ 9.7810e-02, -7.9982e-02,  6.8617e-02],\n",
       "                        [-7.9066e-02, -1.5362e-02, -5.8377e-02]],\n",
       "              \n",
       "                       [[ 5.3863e-02,  6.1136e-02, -9.8569e-02],\n",
       "                        [-9.0493e-02,  1.3877e-02, -2.2676e-03],\n",
       "                        [ 6.1119e-04,  3.2045e-02, -2.0833e-02]],\n",
       "              \n",
       "                       [[ 8.8952e-02,  7.4978e-03, -4.8560e-02],\n",
       "                        [ 3.6576e-02, -8.1784e-02, -5.0412e-02],\n",
       "                        [ 3.0733e-02, -4.4224e-02, -1.0063e-01]],\n",
       "              \n",
       "                       [[ 5.2757e-02,  2.7130e-02,  3.0680e-02],\n",
       "                        [ 3.6639e-02, -1.0402e-01, -4.5922e-02],\n",
       "                        [-5.1204e-02,  1.0086e-01, -4.4409e-02]],\n",
       "              \n",
       "                       [[-6.9249e-02, -3.6996e-02,  1.0128e-02],\n",
       "                        [ 4.2480e-02,  7.7627e-02,  7.6097e-02],\n",
       "                        [ 1.0825e-02,  9.6028e-02,  5.6045e-02]],\n",
       "              \n",
       "                       [[ 5.6241e-02, -9.0389e-02,  8.6117e-03],\n",
       "                        [-2.2388e-02, -1.0017e-01, -5.2057e-02],\n",
       "                        [ 8.0203e-02,  3.8656e-02,  9.7418e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 7.1873e-02, -8.0637e-04,  9.4738e-02],\n",
       "                        [ 2.2233e-02, -1.5484e-02, -4.5076e-02],\n",
       "                        [ 6.7941e-02,  4.7256e-02,  7.3188e-02]],\n",
       "              \n",
       "                       [[-1.8312e-02, -9.7557e-02,  1.9115e-02],\n",
       "                        [ 1.1535e-02, -6.8612e-02,  2.5556e-02],\n",
       "                        [-1.2757e-02,  8.9931e-02,  4.9606e-03]],\n",
       "              \n",
       "                       [[-5.2060e-02,  8.4449e-02,  3.1322e-03],\n",
       "                        [-1.7704e-02,  5.0346e-02,  1.0168e-01],\n",
       "                        [ 4.8064e-02,  2.9723e-03,  8.6332e-02]],\n",
       "              \n",
       "                       [[-4.6228e-02,  9.3158e-02, -6.0058e-02],\n",
       "                        [ 1.9314e-03,  9.5934e-02,  4.1977e-02],\n",
       "                        [-2.2091e-02,  6.4942e-02,  6.2177e-02]],\n",
       "              \n",
       "                       [[-9.5015e-02, -1.1748e-02, -8.2956e-02],\n",
       "                        [-2.2721e-02, -1.2706e-02, -4.2168e-02],\n",
       "                        [-3.8526e-02,  5.7009e-02, -3.1305e-03]],\n",
       "              \n",
       "                       [[-1.0488e-01,  1.8119e-02, -1.0366e-01],\n",
       "                        [-7.2313e-02,  6.4432e-02,  8.0580e-02],\n",
       "                        [-3.7399e-02, -6.4006e-02,  4.7701e-02]],\n",
       "              \n",
       "                       [[-1.0069e-01,  6.5031e-02,  7.2608e-02],\n",
       "                        [ 4.4479e-02,  3.2172e-02,  2.1797e-02],\n",
       "                        [-8.0808e-02, -4.2223e-02,  6.7993e-02]],\n",
       "              \n",
       "                       [[ 8.8781e-02,  2.8155e-03,  2.7081e-02],\n",
       "                        [-9.9981e-02,  7.4412e-02, -6.1396e-02],\n",
       "                        [-5.6505e-02,  1.0401e-01, -3.0216e-02]],\n",
       "              \n",
       "                       [[-5.3254e-02,  1.4500e-02,  8.3070e-02],\n",
       "                        [ 9.5780e-02,  8.8239e-03,  7.1762e-02],\n",
       "                        [ 7.5175e-02, -1.7617e-02, -8.3292e-02]],\n",
       "              \n",
       "                       [[ 2.8359e-03, -3.5807e-02,  3.6940e-02],\n",
       "                        [-7.7232e-02, -7.0704e-02, -7.9384e-02],\n",
       "                        [-7.8857e-02,  8.0180e-02, -1.0503e-01]]]])),\n",
       "             ('conv_block_2.0.bias',\n",
       "              tensor([-0.0039, -0.0651, -0.0443, -0.0304,  0.0495,  0.0709, -0.0237,  0.0034,\n",
       "                      -0.0012,  0.0960])),\n",
       "             ('conv_block_2.2.weight',\n",
       "              tensor([[[[ 4.7112e-02, -2.1886e-02,  7.1378e-02],\n",
       "                        [-8.5113e-02,  4.1778e-02,  8.1767e-02],\n",
       "                        [-5.4711e-02,  3.9443e-02,  4.8332e-02]],\n",
       "              \n",
       "                       [[-8.8435e-02,  3.0322e-02,  7.9715e-02],\n",
       "                        [ 9.6800e-03, -2.0373e-02, -4.1310e-02],\n",
       "                        [-6.9961e-02,  2.5599e-02, -9.9644e-02]],\n",
       "              \n",
       "                       [[-9.0229e-02, -3.8140e-02, -9.9819e-02],\n",
       "                        [ 9.8429e-02, -3.3965e-02, -5.6042e-02],\n",
       "                        [ 2.6497e-02,  4.5364e-02,  7.9380e-02]],\n",
       "              \n",
       "                       [[-4.2556e-02, -1.8419e-02, -7.9599e-02],\n",
       "                        [-9.2043e-02, -4.5865e-02,  1.1860e-02],\n",
       "                        [-5.2256e-02, -2.9098e-02,  3.8637e-02]],\n",
       "              \n",
       "                       [[ 2.1749e-02,  6.5040e-02,  4.8676e-02],\n",
       "                        [ 5.5584e-02, -5.4494e-02,  5.6538e-02],\n",
       "                        [-9.4478e-02,  3.2461e-02, -6.6480e-02]],\n",
       "              \n",
       "                       [[ 2.9107e-02, -9.6440e-02, -1.7458e-02],\n",
       "                        [ 6.4836e-02, -5.2713e-02,  6.5253e-02],\n",
       "                        [-1.6389e-02,  9.8890e-02,  9.9312e-02]],\n",
       "              \n",
       "                       [[-6.2193e-02, -2.4689e-02, -2.1544e-02],\n",
       "                        [-3.9856e-02,  3.4650e-02, -9.5975e-02],\n",
       "                        [-6.3214e-02, -1.6524e-02, -9.3786e-02]],\n",
       "              \n",
       "                       [[-1.0096e-01,  4.3229e-02, -8.8012e-02],\n",
       "                        [ 7.0034e-02, -6.1206e-02, -8.4068e-03],\n",
       "                        [ 6.3559e-02, -1.0395e-01, -1.0263e-01]],\n",
       "              \n",
       "                       [[ 4.3894e-03, -4.2797e-02,  4.9271e-02],\n",
       "                        [ 3.6792e-02,  1.2974e-02,  3.9567e-02],\n",
       "                        [ 9.0181e-02,  1.6730e-02,  6.9743e-02]],\n",
       "              \n",
       "                       [[-5.9431e-02,  8.6851e-02, -1.8743e-02],\n",
       "                        [ 5.7067e-02, -8.7601e-02, -6.3345e-02],\n",
       "                        [-6.1706e-02,  5.5684e-02, -3.5126e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-9.6740e-02,  1.0338e-01,  2.7474e-02],\n",
       "                        [ 9.6950e-02,  1.3158e-02,  1.5111e-02],\n",
       "                        [ 7.5979e-02, -7.1714e-02,  9.9547e-02]],\n",
       "              \n",
       "                       [[-6.7797e-02,  9.5541e-03, -6.8555e-02],\n",
       "                        [-6.9842e-02,  8.1133e-02, -2.7840e-02],\n",
       "                        [-1.9783e-02,  8.2358e-02,  1.0327e-01]],\n",
       "              \n",
       "                       [[-9.5073e-02, -1.8399e-03,  3.0100e-02],\n",
       "                        [ 1.0121e-01, -4.2024e-02, -3.4305e-02],\n",
       "                        [-6.1782e-02,  2.6933e-02,  2.5997e-02]],\n",
       "              \n",
       "                       [[-1.6622e-03, -1.0691e-02, -7.3384e-02],\n",
       "                        [-4.3295e-02, -4.0089e-02,  2.7688e-02],\n",
       "                        [ 4.6549e-02,  4.9211e-02, -8.0461e-02]],\n",
       "              \n",
       "                       [[-8.1253e-03, -2.0117e-02,  1.0515e-01],\n",
       "                        [ 7.6065e-02, -7.8684e-02, -5.7919e-02],\n",
       "                        [-7.9628e-02, -2.6381e-02, -9.2229e-02]],\n",
       "              \n",
       "                       [[ 4.9541e-02, -4.6487e-02, -5.2904e-02],\n",
       "                        [ 4.9818e-03, -3.3680e-02,  7.9631e-02],\n",
       "                        [ 9.5551e-02, -9.4116e-02,  4.7261e-02]],\n",
       "              \n",
       "                       [[ 2.1995e-03,  5.5326e-02, -3.9517e-02],\n",
       "                        [-7.9810e-03, -1.9718e-03,  3.6142e-02],\n",
       "                        [ 5.2557e-02,  4.1110e-02,  1.2937e-02]],\n",
       "              \n",
       "                       [[-8.4090e-02, -6.0552e-02, -6.0051e-02],\n",
       "                        [-1.0505e-01,  1.0097e-01, -4.7723e-02],\n",
       "                        [-7.0065e-02, -4.7735e-02,  5.7234e-02]],\n",
       "              \n",
       "                       [[-6.6079e-02, -8.8077e-02, -7.8786e-02],\n",
       "                        [ 2.3165e-02, -4.3126e-03, -9.5106e-02],\n",
       "                        [-7.2104e-02, -9.1350e-02,  3.6161e-02]],\n",
       "              \n",
       "                       [[ 4.7703e-02,  1.7956e-02, -3.3771e-02],\n",
       "                        [-5.6476e-02,  4.1673e-02, -2.4206e-02],\n",
       "                        [-1.6921e-02, -3.4555e-02,  1.8320e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.4726e-02,  8.6605e-02,  9.3614e-02],\n",
       "                        [-2.7167e-02,  1.5666e-02, -7.0456e-02],\n",
       "                        [-7.9131e-02,  8.7409e-02,  1.2417e-02]],\n",
       "              \n",
       "                       [[-5.2411e-02,  5.2994e-02,  4.2525e-02],\n",
       "                        [ 3.7733e-02,  4.4601e-02,  5.6296e-03],\n",
       "                        [-7.6680e-02, -9.3707e-02,  1.0291e-01]],\n",
       "              \n",
       "                       [[ 7.3588e-02, -8.2237e-02,  8.2604e-02],\n",
       "                        [-9.8587e-02,  7.0118e-02,  7.3401e-02],\n",
       "                        [ 6.6224e-02,  4.4611e-02, -6.3845e-02]],\n",
       "              \n",
       "                       [[-4.5205e-02,  2.2597e-02,  4.1573e-02],\n",
       "                        [ 9.2402e-02,  4.6889e-03,  4.3407e-02],\n",
       "                        [-7.9607e-02, -2.1395e-02,  6.2686e-02]],\n",
       "              \n",
       "                       [[ 8.6152e-03,  8.3330e-02, -8.4583e-02],\n",
       "                        [-7.0335e-02,  3.3138e-02, -4.4079e-03],\n",
       "                        [-9.0067e-02,  4.4976e-02,  4.3445e-03]],\n",
       "              \n",
       "                       [[ 2.5491e-02,  2.3164e-02, -7.2684e-02],\n",
       "                        [-7.0707e-02, -8.4338e-02,  7.6564e-02],\n",
       "                        [-5.7863e-02,  9.4893e-02, -3.3662e-02]],\n",
       "              \n",
       "                       [[ 2.3332e-02,  1.0067e-01, -5.6567e-02],\n",
       "                        [-9.9979e-02,  1.5201e-02, -7.6046e-02],\n",
       "                        [ 5.8083e-02,  4.1906e-02,  7.6373e-02]],\n",
       "              \n",
       "                       [[-1.0950e-02, -6.8867e-02, -9.7139e-02],\n",
       "                        [-6.8084e-02,  9.3890e-02, -2.2565e-02],\n",
       "                        [-1.9308e-03, -2.2366e-02, -5.3820e-02]],\n",
       "              \n",
       "                       [[ 8.6292e-02, -2.7310e-02, -5.1159e-02],\n",
       "                        [ 9.7393e-02,  6.3091e-02,  5.6178e-02],\n",
       "                        [ 7.5538e-02, -6.6939e-02,  3.4327e-02]],\n",
       "              \n",
       "                       [[-2.9337e-02,  3.7678e-03,  4.9652e-02],\n",
       "                        [-3.4990e-02,  1.0343e-01,  5.1694e-04],\n",
       "                        [ 6.0737e-02,  9.7003e-03, -1.0160e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.3044e-02, -8.7835e-02,  1.2386e-02],\n",
       "                        [-7.0713e-02, -8.3828e-02,  2.2599e-02],\n",
       "                        [-9.2269e-02, -5.7359e-02, -6.8190e-02]],\n",
       "              \n",
       "                       [[ 5.1452e-02,  9.6645e-02,  3.6881e-02],\n",
       "                        [-5.8994e-02,  7.9785e-02,  9.7840e-02],\n",
       "                        [-2.8560e-02,  7.7781e-02,  2.6648e-02]],\n",
       "              \n",
       "                       [[-7.3321e-02, -2.7531e-02, -8.4067e-02],\n",
       "                        [ 3.1805e-02,  5.8290e-02, -1.0368e-01],\n",
       "                        [-4.3082e-02,  2.2843e-03,  8.4405e-02]],\n",
       "              \n",
       "                       [[-5.7436e-02, -6.1907e-03, -2.9809e-03],\n",
       "                        [-5.3008e-02, -4.7247e-02, -2.8203e-02],\n",
       "                        [-3.1497e-02, -3.3929e-02,  1.0078e-02]],\n",
       "              \n",
       "                       [[-8.8121e-03, -5.1300e-02,  6.3834e-02],\n",
       "                        [ 7.5409e-02, -1.6524e-02, -1.9898e-02],\n",
       "                        [-1.0274e-01, -7.7062e-02, -4.8009e-03]],\n",
       "              \n",
       "                       [[-2.0096e-02,  3.0839e-02, -5.2436e-02],\n",
       "                        [-2.1832e-02,  1.5762e-02,  1.0347e-01],\n",
       "                        [ 8.2933e-02, -6.9338e-02, -8.7794e-03]],\n",
       "              \n",
       "                       [[-1.7258e-02, -3.3695e-03,  4.3913e-02],\n",
       "                        [-4.2404e-02, -2.2469e-02, -1.0442e-01],\n",
       "                        [-3.7445e-02, -1.0338e-01,  3.7316e-02]],\n",
       "              \n",
       "                       [[-8.1140e-02, -5.2087e-02, -7.6440e-02],\n",
       "                        [ 8.0156e-02, -3.0944e-02,  9.1603e-02],\n",
       "                        [-5.2379e-02,  7.9800e-02, -2.0726e-02]],\n",
       "              \n",
       "                       [[-7.9184e-02,  8.8149e-02, -7.1711e-02],\n",
       "                        [-6.1986e-02,  5.7765e-02, -9.8403e-03],\n",
       "                        [-1.6010e-03,  1.0452e-01, -8.3651e-02]],\n",
       "              \n",
       "                       [[-3.8073e-02,  4.2882e-02, -2.0297e-02],\n",
       "                        [ 1.7831e-03, -3.5636e-02, -1.6329e-02],\n",
       "                        [ 8.4139e-02, -1.9594e-02,  4.2393e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 7.9533e-02,  8.1603e-02,  4.9980e-02],\n",
       "                        [ 7.2456e-02, -2.7342e-02,  8.4049e-03],\n",
       "                        [-4.5216e-02, -1.0223e-01,  8.8693e-02]],\n",
       "              \n",
       "                       [[-1.0077e-01, -6.6432e-02, -3.7569e-02],\n",
       "                        [-9.6421e-02, -8.1801e-02, -5.5137e-02],\n",
       "                        [ 1.2473e-02, -9.2162e-02, -7.5770e-02]],\n",
       "              \n",
       "                       [[-6.1618e-02, -7.9265e-02,  2.4742e-02],\n",
       "                        [ 3.5664e-03,  2.3131e-02,  9.5094e-02],\n",
       "                        [-4.6153e-02, -3.9520e-02,  3.3627e-02]],\n",
       "              \n",
       "                       [[ 9.2449e-02,  3.3643e-02, -9.0360e-03],\n",
       "                        [ 3.4240e-02, -1.0383e-01, -7.2103e-02],\n",
       "                        [-4.4324e-02, -9.5421e-02, -7.9608e-02]],\n",
       "              \n",
       "                       [[-8.1068e-02,  3.0260e-02,  3.1935e-02],\n",
       "                        [-2.5388e-03, -2.4364e-02,  2.8421e-03],\n",
       "                        [ 6.3693e-02, -7.9371e-02,  7.4888e-02]],\n",
       "              \n",
       "                       [[ 4.5845e-02, -3.6795e-02,  7.3957e-02],\n",
       "                        [-8.7307e-05, -4.8306e-02,  9.4993e-02],\n",
       "                        [ 3.1155e-02,  7.9549e-02, -2.8604e-02]],\n",
       "              \n",
       "                       [[-9.0326e-02,  8.4532e-02,  6.0980e-03],\n",
       "                        [ 5.0060e-02, -2.7322e-02,  4.9856e-02],\n",
       "                        [-8.3288e-02,  3.7182e-03,  8.0037e-02]],\n",
       "              \n",
       "                       [[-2.8305e-02,  9.0535e-02,  3.8764e-02],\n",
       "                        [ 5.1963e-02,  1.0416e-01, -2.5647e-02],\n",
       "                        [-2.4540e-02,  6.6285e-02, -5.3344e-02]],\n",
       "              \n",
       "                       [[-7.7720e-02,  6.8732e-02, -9.1971e-02],\n",
       "                        [-4.9922e-02,  2.5541e-02,  1.5522e-02],\n",
       "                        [-6.3903e-02, -2.6667e-02,  6.7663e-02]],\n",
       "              \n",
       "                       [[-6.8302e-02,  6.0463e-02, -1.1288e-02],\n",
       "                        [-3.7731e-02, -1.0041e-01, -2.7648e-02],\n",
       "                        [ 8.6676e-02, -5.7746e-02, -8.3657e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-8.2241e-03,  7.4723e-02, -2.8593e-02],\n",
       "                        [ 1.0333e-01,  1.0153e-01,  9.6574e-02],\n",
       "                        [-6.0292e-02, -7.5035e-02, -9.8788e-02]],\n",
       "              \n",
       "                       [[-2.0497e-02, -6.8647e-02,  4.9107e-02],\n",
       "                        [ 9.5066e-03,  8.3601e-02, -9.7785e-02],\n",
       "                        [-8.8578e-02, -1.9616e-02,  2.7782e-02]],\n",
       "              \n",
       "                       [[ 8.8749e-02, -8.8129e-03,  9.0081e-02],\n",
       "                        [-2.8536e-02, -1.4880e-02, -4.0588e-02],\n",
       "                        [-4.4799e-02, -3.9932e-02, -9.1118e-02]],\n",
       "              \n",
       "                       [[-1.2940e-02, -3.5543e-02, -6.8246e-02],\n",
       "                        [-7.8187e-02, -1.0165e-01, -2.3621e-02],\n",
       "                        [-8.5819e-02,  9.4773e-02, -2.8295e-02]],\n",
       "              \n",
       "                       [[ 2.8334e-02, -7.5572e-02,  1.0756e-02],\n",
       "                        [-9.9068e-02,  4.7454e-02,  4.9781e-02],\n",
       "                        [-1.0243e-01,  4.5965e-02,  1.3420e-02]],\n",
       "              \n",
       "                       [[-6.5835e-02, -4.8260e-02,  1.0361e-01],\n",
       "                        [-5.4829e-02,  9.2227e-02,  2.1213e-02],\n",
       "                        [ 4.8295e-02, -5.3057e-02,  3.4265e-02]],\n",
       "              \n",
       "                       [[ 2.2697e-02,  8.4069e-02, -1.0424e-01],\n",
       "                        [-3.8324e-02,  2.6248e-03, -7.5657e-02],\n",
       "                        [ 1.7964e-02,  6.7965e-02,  6.3523e-02]],\n",
       "              \n",
       "                       [[-5.4354e-02,  4.2609e-02, -7.6699e-02],\n",
       "                        [ 6.5526e-02, -8.6154e-02, -9.5563e-02],\n",
       "                        [ 3.6582e-02, -9.9508e-03,  2.5983e-02]],\n",
       "              \n",
       "                       [[ 9.8082e-02,  1.0098e-01,  5.9435e-02],\n",
       "                        [-9.5619e-02, -1.6229e-02,  1.1523e-02],\n",
       "                        [-1.0264e-01,  8.8152e-02,  6.9171e-02]],\n",
       "              \n",
       "                       [[-4.8693e-02, -2.1999e-02, -6.1142e-02],\n",
       "                        [ 4.0543e-02,  1.8179e-02, -3.6544e-02],\n",
       "                        [ 5.6355e-02, -2.2786e-02,  1.1320e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 7.2982e-02, -4.9525e-02, -9.7219e-02],\n",
       "                        [ 5.6892e-02, -6.8888e-02,  3.9849e-02],\n",
       "                        [-3.1690e-02,  7.0759e-02, -4.0092e-03]],\n",
       "              \n",
       "                       [[-4.7994e-02, -7.4361e-02,  7.0832e-02],\n",
       "                        [-6.5956e-02,  2.9881e-02,  8.0612e-02],\n",
       "                        [-8.4673e-02, -8.8470e-02,  1.0754e-02]],\n",
       "              \n",
       "                       [[-4.1378e-02, -7.7379e-02,  6.9702e-02],\n",
       "                        [-9.8059e-02, -4.8061e-02, -8.7558e-02],\n",
       "                        [ 2.7860e-02,  9.0914e-03,  4.3968e-02]],\n",
       "              \n",
       "                       [[ 4.4132e-02, -4.1697e-04, -3.1655e-02],\n",
       "                        [ 9.3207e-02,  4.3835e-03,  2.5096e-02],\n",
       "                        [-8.6797e-02, -1.0331e-01, -5.1486e-02]],\n",
       "              \n",
       "                       [[-5.5235e-02,  3.0540e-02, -4.6851e-02],\n",
       "                        [ 1.1008e-02,  9.7682e-02, -3.1095e-02],\n",
       "                        [ 2.6931e-02,  1.2927e-03,  6.9368e-02]],\n",
       "              \n",
       "                       [[ 4.6102e-02, -4.8972e-03,  5.7655e-02],\n",
       "                        [-5.3630e-03, -1.7920e-02, -4.1088e-02],\n",
       "                        [-8.5991e-02,  8.8051e-02, -6.6798e-02]],\n",
       "              \n",
       "                       [[-1.0049e-01, -4.7279e-02,  2.1293e-02],\n",
       "                        [-9.5659e-03,  4.7986e-02,  6.0392e-02],\n",
       "                        [ 6.1992e-02, -8.8441e-02,  4.2975e-02]],\n",
       "              \n",
       "                       [[ 5.2178e-02, -5.8477e-02,  5.1058e-04],\n",
       "                        [ 1.5799e-02,  1.3218e-02,  5.6564e-02],\n",
       "                        [ 8.9493e-02,  1.0124e-01,  2.3601e-02]],\n",
       "              \n",
       "                       [[ 6.1643e-02,  4.5068e-02, -5.7026e-02],\n",
       "                        [ 7.0499e-02, -7.4894e-03,  4.3017e-02],\n",
       "                        [-6.9960e-02,  8.4333e-02,  2.3526e-02]],\n",
       "              \n",
       "                       [[ 2.3887e-02, -9.3772e-02, -9.1610e-03],\n",
       "                        [-3.6860e-02,  9.9519e-02, -9.6504e-02],\n",
       "                        [-2.6012e-02, -5.2282e-02, -7.6413e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.0542e-02, -7.2777e-02,  4.9827e-02],\n",
       "                        [ 1.2652e-02, -5.5806e-02, -1.1996e-02],\n",
       "                        [ 9.1412e-03, -4.6977e-02, -6.7726e-02]],\n",
       "              \n",
       "                       [[-5.9402e-02,  2.5433e-02, -9.2880e-02],\n",
       "                        [ 6.6374e-02,  6.9227e-02,  9.6927e-02],\n",
       "                        [ 8.9183e-02, -5.9241e-02, -4.5740e-02]],\n",
       "              \n",
       "                       [[ 7.7152e-02, -1.3636e-02, -7.7610e-02],\n",
       "                        [-5.0493e-02,  7.3992e-02, -5.9942e-02],\n",
       "                        [ 3.7075e-02, -8.8013e-02,  6.9605e-02]],\n",
       "              \n",
       "                       [[ 2.8992e-02,  7.2709e-02,  3.5951e-03],\n",
       "                        [ 5.3507e-02,  8.6381e-02, -1.3297e-02],\n",
       "                        [-7.9664e-02, -7.4377e-02,  6.1803e-02]],\n",
       "              \n",
       "                       [[-6.7218e-02, -9.2271e-02, -8.9252e-02],\n",
       "                        [-4.6682e-02,  1.9555e-02, -9.5512e-02],\n",
       "                        [-4.8844e-02,  1.0092e-01, -1.7928e-02]],\n",
       "              \n",
       "                       [[-3.5571e-02,  4.9854e-02, -8.6741e-02],\n",
       "                        [-8.2307e-02,  1.5071e-02,  3.3322e-02],\n",
       "                        [-1.0388e-01,  9.2613e-02, -4.1045e-02]],\n",
       "              \n",
       "                       [[ 1.6103e-02,  7.4930e-02,  4.0348e-02],\n",
       "                        [ 3.2730e-02, -9.6173e-02, -9.8345e-02],\n",
       "                        [ 3.9202e-02,  3.9173e-03,  6.4725e-02]],\n",
       "              \n",
       "                       [[ 5.2750e-02, -8.4565e-02,  1.9401e-02],\n",
       "                        [-8.2694e-02, -5.8268e-02,  2.8968e-02],\n",
       "                        [-5.8921e-03, -6.7319e-02,  3.5743e-02]],\n",
       "              \n",
       "                       [[ 7.3149e-02, -1.0359e-01,  9.8872e-02],\n",
       "                        [ 5.8189e-02, -8.8846e-03, -1.0222e-01],\n",
       "                        [-4.4622e-02,  3.2169e-02, -4.4910e-02]],\n",
       "              \n",
       "                       [[-3.4649e-02, -4.3531e-02,  9.0629e-02],\n",
       "                        [ 8.9208e-02, -8.2216e-02, -3.2520e-02],\n",
       "                        [ 9.5476e-02, -6.0714e-03, -5.1321e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.2747e-02,  8.3403e-02, -1.3414e-02],\n",
       "                        [-9.9466e-02, -3.5235e-02,  5.2461e-02],\n",
       "                        [-1.5575e-02,  7.4476e-02,  3.6252e-02]],\n",
       "              \n",
       "                       [[ 9.2444e-04,  4.0264e-02, -1.4228e-02],\n",
       "                        [-9.0362e-02, -9.0336e-02, -8.2914e-02],\n",
       "                        [-1.6789e-02, -7.6217e-02,  1.5448e-02]],\n",
       "              \n",
       "                       [[ 1.2086e-02,  8.0204e-02,  5.9270e-03],\n",
       "                        [ 1.7966e-02,  3.7482e-02, -2.9801e-02],\n",
       "                        [-5.2524e-02, -9.9156e-02, -1.0266e-01]],\n",
       "              \n",
       "                       [[ 6.9312e-03, -4.9847e-02,  2.9181e-02],\n",
       "                        [ 2.6197e-02,  8.2212e-02, -1.2868e-02],\n",
       "                        [-4.4575e-04,  7.2751e-02,  1.7875e-02]],\n",
       "              \n",
       "                       [[ 4.8244e-02, -5.6920e-02, -7.6188e-02],\n",
       "                        [ 8.0363e-02,  7.5140e-02, -3.9912e-02],\n",
       "                        [ 1.7510e-02, -9.3745e-02,  7.3770e-03]],\n",
       "              \n",
       "                       [[-9.7866e-02,  5.9578e-02, -1.0448e-01],\n",
       "                        [-2.5427e-02,  9.8347e-02, -3.5711e-02],\n",
       "                        [-6.1100e-02,  9.5090e-02, -5.3318e-02]],\n",
       "              \n",
       "                       [[ 2.3248e-02,  5.8911e-02,  2.8321e-02],\n",
       "                        [-7.2826e-02,  9.5668e-02,  5.0546e-02],\n",
       "                        [-6.6285e-02, -6.0160e-02, -2.4312e-02]],\n",
       "              \n",
       "                       [[ 3.5167e-02,  3.9857e-02,  1.3573e-02],\n",
       "                        [ 6.9129e-02,  1.0486e-01,  2.1686e-02],\n",
       "                        [ 4.1288e-02,  5.9946e-02,  2.0896e-02]],\n",
       "              \n",
       "                       [[ 6.4990e-03,  9.4775e-02,  7.2772e-02],\n",
       "                        [ 6.8846e-02, -4.1016e-02, -5.0397e-02],\n",
       "                        [-5.7157e-02, -8.8271e-02, -1.0710e-02]],\n",
       "              \n",
       "                       [[ 5.9391e-02,  1.5443e-02, -3.7287e-02],\n",
       "                        [-1.4582e-02, -8.0778e-02, -6.7709e-02],\n",
       "                        [ 9.7044e-03, -6.1245e-02,  1.2991e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 8.6932e-02,  2.6993e-02,  8.0693e-02],\n",
       "                        [-6.6783e-02,  2.7414e-03, -1.0355e-01],\n",
       "                        [ 7.3036e-04, -7.7652e-02,  9.8931e-02]],\n",
       "              \n",
       "                       [[-9.0113e-02,  3.6033e-02, -6.3054e-02],\n",
       "                        [ 2.9178e-02, -5.4980e-04, -1.3482e-02],\n",
       "                        [-4.0910e-02,  6.7776e-02, -1.9027e-02]],\n",
       "              \n",
       "                       [[ 8.9846e-03, -3.1766e-02, -7.6815e-02],\n",
       "                        [-4.3930e-02,  9.4509e-02,  1.0034e-01],\n",
       "                        [-4.8973e-02,  1.0505e-02,  1.7110e-02]],\n",
       "              \n",
       "                       [[ 9.2989e-02,  1.4710e-02,  4.9070e-03],\n",
       "                        [-3.0662e-02,  2.2230e-02, -1.0495e-01],\n",
       "                        [-1.0517e-01, -1.2219e-02, -7.3111e-02]],\n",
       "              \n",
       "                       [[-7.3207e-02, -1.7892e-02,  5.8483e-02],\n",
       "                        [ 5.2169e-02, -3.6623e-02, -7.4490e-03],\n",
       "                        [ 9.7855e-02,  2.2802e-02,  7.1682e-02]],\n",
       "              \n",
       "                       [[-4.3661e-02, -9.4329e-02, -4.8872e-02],\n",
       "                        [ 9.2634e-03, -2.5530e-02, -5.7555e-02],\n",
       "                        [ 1.0137e-01, -9.7296e-02,  2.6763e-03]],\n",
       "              \n",
       "                       [[ 4.2008e-02, -2.8438e-02,  4.5717e-02],\n",
       "                        [ 3.7417e-04, -6.2659e-02, -5.7317e-02],\n",
       "                        [-4.7419e-02, -1.0292e-01,  6.5697e-02]],\n",
       "              \n",
       "                       [[ 9.6623e-02, -5.4148e-02,  5.2101e-03],\n",
       "                        [ 3.5223e-02,  2.9846e-02, -5.8507e-02],\n",
       "                        [-6.4055e-02,  1.0261e-01,  6.1382e-02]],\n",
       "              \n",
       "                       [[ 9.8538e-02,  7.3996e-02, -7.8429e-03],\n",
       "                        [ 7.4208e-02, -3.3809e-03, -6.5400e-04],\n",
       "                        [-4.5672e-02,  8.2204e-02, -2.2410e-02]],\n",
       "              \n",
       "                       [[ 1.5304e-02, -8.4053e-02,  3.6159e-02],\n",
       "                        [-5.0118e-02, -2.3802e-02, -9.4259e-03],\n",
       "                        [ 3.9077e-02,  4.3028e-03,  1.8367e-02]]]])),\n",
       "             ('conv_block_2.2.bias',\n",
       "              tensor([-0.0685, -0.0796,  0.0380, -0.0434, -0.0909, -0.0435, -0.1034,  0.0211,\n",
       "                       0.0078,  0.0973])),\n",
       "             ('classifier.1.weight',\n",
       "              tensor([[-0.0249, -0.0234,  0.0108,  ..., -0.0122,  0.0273,  0.0140],\n",
       "                      [-0.0037,  0.0287,  0.0322,  ..., -0.0399, -0.0342,  0.0331],\n",
       "                      [-0.0262, -0.0221, -0.0107,  ..., -0.0422,  0.0242, -0.0155],\n",
       "                      ...,\n",
       "                      [-0.0385, -0.0342,  0.0408,  ..., -0.0178, -0.0026, -0.0298],\n",
       "                      [ 0.0152, -0.0113,  0.0421,  ...,  0.0409,  0.0064, -0.0336],\n",
       "                      [ 0.0120, -0.0291, -0.0328,  ...,  0.0195, -0.0131, -0.0308]])),\n",
       "             ('classifier.1.bias',\n",
       "              tensor([ 0.0071,  0.0436,  0.0445,  0.0371, -0.0339,  0.0299,  0.0083, -0.0367,\n",
       "                       0.0029, -0.0436]))])"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helper_functions import accuracy_fn\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params = model_2.parameters(), lr=0.01 )\n",
    "\n",
    "\n",
    "model_2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is the 0 number\n",
      "the train loss is 2.30866     || \n",
      "the train accuracy is 9.37500     || \n",
      "the train loss is 2.30258     || \n",
      "the train accuracy is 9.37500     || \n",
      "the train loss is 2.02019     || \n",
      "the train accuracy is 28.12500     || \n",
      "the train loss is 0.71729     || \n",
      "the train accuracy is 81.25000     || \n",
      "the train loss is 0.72450     || \n",
      "the train accuracy is 78.12500     || \n",
      "the test loss is 0.69186     || \n",
      " test accuracy is 71.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:35<01:10, 35.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is the 1 number\n",
      "the train loss is 0.89494     || \n",
      "the train accuracy is 68.75000     || \n",
      "the train loss is 0.57345     || \n",
      "the train accuracy is 78.12500     || \n",
      "the train loss is 0.46073     || \n",
      "the train accuracy is 81.25000     || \n",
      "the train loss is 0.47695     || \n",
      "the train accuracy is 81.25000     || \n",
      "the train loss is 0.33488     || \n",
      "the train accuracy is 90.62500     || \n",
      "the test loss is 0.56839     || \n",
      " test accuracy is 71.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [01:34<00:49, 49.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is the 2 number\n",
      "the train loss is 0.42094     || \n",
      "the train accuracy is 90.62500     || \n",
      "the train loss is 0.26523     || \n",
      "the train accuracy is 90.62500     || \n",
      "the train loss is 0.52114     || \n",
      "the train accuracy is 84.37500     || \n",
      "the train loss is 0.23103     || \n",
      "the train accuracy is 93.75000     || \n",
      "the train loss is 0.34670     || \n",
      "the train accuracy is 84.37500     || \n",
      "the test loss is 0.70320     || \n",
      " test accuracy is 75.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [02:31<00:00, 50.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the elapsed time is 151.40154361724854 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## run the training and test loop\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    print(f\" this is the {epoch} number\")\n",
    "\n",
    "    training_step( model= model_2,\n",
    "                  optimizer= optimizer,\n",
    "                  loss = loss_fn,\n",
    "                  accuracy_fn = accuracy_fn,\n",
    "                  dataloader= train_data_batch,\n",
    "                  device= 'cpu')\n",
    "    \n",
    "\n",
    "    test_step( model= model_2,\n",
    "              loss_fn= loss_fn,\n",
    "              accuracy_fn= accuracy_fn,\n",
    "              dataloader= test_data_batch,\n",
    "              device= 'cpu')\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "print(my_run_time(start_time = start_time, end_time= end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation model\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def eval_model(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data,\n",
    "               accuracy_fn,\n",
    "               loss_fn: nn.Module,\n",
    "               device: torch.device = device):\n",
    "    \n",
    "    acc, loss = 0, 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X,y) in enumerate(dataloader):\n",
    "            # define forward pass\n",
    "            y_test = model(X)\n",
    "\n",
    "            #loss fn and accuracy function\n",
    "            loss += loss_fn(y_test, y)\n",
    "\n",
    "            acc += accuracy_fn(y_true= y, y_pred= y_test.argmax(dim=1))\n",
    "\n",
    "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making predictions based on the \n",
    "\n",
    "def make_predictions(model: nn.Module,\n",
    "                     data: list,\n",
    "                     device: torch.device = device,\n",
    "                     ):\n",
    "    \n",
    "    pred_probs = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        for sample in data:\n",
    "\n",
    "            #make a forward pass\n",
    "\n",
    "            y_preds = model(sample)\n",
    "\n",
    "            #turn the predictions to probabilities\n",
    "\n",
    "            pred_prob = torch.softmax(y_preds.squeeze(), dim=0)\n",
    "\n",
    "            #append to list\n",
    "\n",
    "            pred_probs.append(pred_prob)\n",
    "\n",
    "    return torch.stack(pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "#prepare data\n",
    "test_sample = []\n",
    "test_label = []\n",
    "\n",
    "for sample, label in random.sample(list(test_data), k = 9):\n",
    "    test_sample.append(sample.unsqueeze(1))\n",
    "    test_label.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5, 1, 7, 4, 3, 0, 4, 7, 1]), [5, 1, 7, 4, 3, 0, 4, 7, 1])"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make preduictions using our prediction function\n",
    "\n",
    "predi_probs = make_predictions(model_2, data= test_sample, device= 'cpu')\n",
    "\n",
    "#convert to prediction probs\n",
    "\n",
    "prediction_probs = predi_probs.argmax(dim=1)\n",
    "\n",
    "prediction_probs, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
